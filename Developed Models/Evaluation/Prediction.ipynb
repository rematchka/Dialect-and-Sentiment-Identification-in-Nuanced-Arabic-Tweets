{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c784b93",
   "metadata": {},
   "source": [
    "# Importing Libariries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91ee6dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from transformers import AutoModel\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "# Any results you write to the current directory are saved as output.\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "from transformers import BertTokenizer,BertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from argparse import ArgumentParser\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "from ignite.engine.engine import Engine, State, Events\n",
    "from ignite.handlers import EarlyStopping\n",
    "from ignite.contrib.handlers import TensorboardLogger, ProgressBar\n",
    "from ignite.utils import convert_tensor\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import warnings  \n",
    "warnings.filterwarnings('ignore')\n",
    "import gc\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "\n",
    "# For data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Pytorch Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Utils\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# Sklearn Imports\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW\n",
    "import random\n",
    "import os\n",
    "from urllib import request\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_score , recall_score\n",
    "\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, BertTokenizer\n",
    "from transformers.data.processors import SingleSentenceClassificationProcessor\n",
    "from transformers import Trainer , TrainingArguments\n",
    "from transformers.trainer_utils import EvaluationStrategy\n",
    "from transformers.data.processors.utils import InputFeatures\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "142c6003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, PreTrainedModel\n",
    "from transformers import BertModel, BertPreTrainedModel, BertForMaskedLM\n",
    "from transformers.models.bert.modeling_bert import BertOnlyMLMHead, BertLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f58f3bb",
   "metadata": {},
   "source": [
    "# Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "078066eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDatasetFeatures(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length):\n",
    "        self.df = df\n",
    "        self.max_len = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text = df['text'].values\n",
    "        self.fetures = df[['POS_1', 'POS_2', 'POS_3', 'POS_4',\n",
    "       'Positive Sentiment', 'Negative Sentiment', 'sentiment',\n",
    "       'Blob Polarity', 'Blob Subjectivity', 'positive Sentiment first half',\n",
    "       'negative Sentiment first half', 'first half sentiment',\n",
    "       'first half Blob Polarity', 'first half Blob Subjectivity',\n",
    "       'positive Sentiment second half', 'negative Sentiment second half',\n",
    "       'second half sentiment', 'second half Blob Polarity',\n",
    "       'second half Blob Subjectivity',  'Topic :',\n",
    "       'contain_emoji', 'count_special_chracter', 'count_question_marks',\n",
    "       'subjectivity', 'count_verbs', 'count_nouns', 'count_pronun',\n",
    "       'count_adjct', 'count_profane']].astype(float).values\n",
    "       \n",
    "#         self.label=df['label'].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.text[index]\n",
    "        # summary = self.summary[index]\n",
    "        inputs_text = self.tokenizer.encode_plus(\n",
    "                                text,\n",
    "                                truncation=True,\n",
    "                                add_special_tokens=True,\n",
    "                                max_length=self.max_len,\n",
    "                                padding='max_length'\n",
    "                            )\n",
    "        \n",
    "                            \n",
    "#         target = self.label[index]\n",
    "        \n",
    "        text_ids = inputs_text['input_ids']\n",
    "        text_mask = inputs_text['attention_mask']\n",
    "        token_type_ids=inputs_text['token_type_ids']\n",
    "       \n",
    "        \n",
    "        \n",
    "        return {\n",
    "            \n",
    "            'input_ids': torch.tensor(text_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(text_mask, dtype=torch.long),\n",
    "            'features': torch.tensor(self.fetures[index], dtype=torch.float),\n",
    "             'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "#             'target': torch.tensor(target, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "078ee6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=512):\n",
    "        self.df = df\n",
    "        self.max_len = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text = df['text'].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.text[index]\n",
    "        # summary = self.summary[index]\n",
    "        inputs_text = self.tokenizer.encode_plus(\n",
    "                                text,\n",
    "                                truncation=True,\n",
    "                                add_special_tokens=True,\n",
    "                                max_length=self.max_len,\n",
    "                                padding='max_length'\n",
    "                            )\n",
    "        \n",
    "                            \n",
    "        \n",
    "        text_ids = inputs_text['input_ids']\n",
    "        text_mask = inputs_text['attention_mask']\n",
    "        token_type_ids=inputs_text['token_type_ids']\n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "        return {\n",
    "            \n",
    "            'input_ids': torch.tensor(text_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(text_mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1574b208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c441907",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03c535a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score,f1_score,accuracy_score,recall_score,precision_score,classification_report\n",
    "def print_statistics(y, y_pred):\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    precision =precision_score(y, y_pred, average='weighted')\n",
    "    recall = recall_score(y, y_pred, average='weighted')\n",
    "    f_score = f1_score(y, y_pred, average='weighted')\n",
    "    print('Accuracy: %.3f\\nPrecision: %.3f\\nRecall: %.3f\\nF_score: %.3f\\n'\n",
    "          % (accuracy, precision, recall, f_score))\n",
    "    print(classification_report(y, y_pred))\n",
    "    return accuracy, precision, recall, f_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4c5e1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from transformers import AutoModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def init_weights(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv2d') != -1 or classname.find('ConvTranspose2d') != -1:\n",
    "        nn.init.kaiming_uniform_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "class AttentionWithContext(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(AttentionWithContext, self).__init__()\n",
    "\n",
    "        self.attn = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.contx = nn.Linear(hidden_dim, 1, bias=False)\n",
    "        #self.apply(init_weights)\n",
    "    def forward(self, inp):\n",
    "        u = torch.tanh_(self.attn(inp))\n",
    "        a = F.softmax(self.contx(u))\n",
    "        s = (a * inp).sum(1)\n",
    "        return s\n",
    "\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 pretrained_path='aubmindlab/bert-base-arabert'):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "\n",
    "        \n",
    "        self.transformer = AutoModel.from_pretrained(pretrained_path, output_hidden_states=True)\n",
    "\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None):\n",
    "        outputs = self.transformer(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "            , output_hidden_states=True\n",
    "        )\n",
    "        #(output_last_layer, pooled_cls, (output_layers))\n",
    "        #output[0] (8, seqlen=64, 768) cls [8, 768] ( 12 (8, seqlen=64, 768))\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def output_num(self):\n",
    "        return self.transformer.config.hidden_size\n",
    "\n",
    "class ATTClassifier(nn.Module):\n",
    "    def __init__(self, in_feature, class_num=1, dropout_prob=0.2):\n",
    "        super(ATTClassifier, self).__init__()\n",
    "        self.attention = AttentionWithContext(in_feature)\n",
    "\n",
    "        self.Classifier = nn.Sequential(\n",
    "            nn.Linear(2 * in_feature, 512),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, class_num)\n",
    "        )\n",
    "\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        att = self.attention(x[0]) #(X[0] (bs, seqlenght, embedD) att = \\sum_i alpha_i x[0][i]\n",
    "\n",
    "        xx = torch.cat([att, x[1]], 1)\n",
    "\n",
    "        out = self.Classifier(xx)\n",
    "        return out\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        \n",
    "        self.supports_masking = True\n",
    "\n",
    "        self.bias = bias\n",
    "        self.feature_dim = feature_dim\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        \n",
    "        weight = torch.zeros(feature_dim, 1)\n",
    "        nn.init.kaiming_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "        \n",
    "        if bias:\n",
    "            self.b = nn.Parameter(torch.zeros(step_dim))\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        feature_dim = self.feature_dim \n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = torch.mm(\n",
    "            x.contiguous().view(-1, feature_dim), \n",
    "            self.weight\n",
    "        ).view(-1, step_dim)\n",
    "        \n",
    "        if self.bias:\n",
    "            eij = eij + self.b\n",
    "            \n",
    "        eij = torch.tanh(eij)\n",
    "        a = torch.exp(eij)\n",
    "        \n",
    "        if mask is not None:\n",
    "            a = a * mask\n",
    "\n",
    "        a = a / (torch.sum(a, 1, keepdim=True) + 1e-10)\n",
    "\n",
    "        weighted_input = x * torch.unsqueeze(a, -1)\n",
    "        return torch.sum(weighted_input, 1)\n",
    "\n",
    "class WeightedLayerPooling(nn.Module):\n",
    "    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights=None):\n",
    "        super(WeightedLayerPooling, self).__init__()\n",
    "        self.layer_start = layer_start\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.layer_weights = layer_weights if layer_weights is not None \\\n",
    "            else nn.Parameter(\n",
    "            torch.tensor([1] * (num_hidden_layers + 1 - layer_start), dtype=torch.float)\n",
    "        )\n",
    "\n",
    "    def forward(self, all_hidden_states):\n",
    "        all_layer_embedding = all_hidden_states[self.layer_start:, :, :, :]\n",
    "        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n",
    "        weighted_average = (weight_factor * all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n",
    "        return weighted_average\n",
    "class NADIModelLSTMFeatures(nn.Module):\n",
    "  def __init__(self, pretrained_path='aubmindlab/bert-base-arabert',in_feature=768, class_num=3, dropout_prob=0.2):\n",
    "        super(NADIModelLSTMFeatures, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(pretrained_path, output_hidden_states=True)\n",
    "#         freeze(self.bert.embeddings)\n",
    "#         freeze(self.bert.encoder.layer[:4])\n",
    "            \n",
    "        self.hidden_size = self.bert.config.hidden_size\n",
    "        self.LSTM_1 = nn.LSTM(self.hidden_size,self.hidden_size,bidirectional=True)\n",
    "        self.LSTM_2 = nn.LSTM(self.hidden_size,self.hidden_size,bidirectional=True)\n",
    "        self.LSTM_3 = nn.LSTM(self.hidden_size,self.hidden_size,bidirectional=True)\n",
    "        self.LSTM_4 = nn.LSTM(self.hidden_size,self.hidden_size,bidirectional=True)\n",
    "        self.clf_1 = nn.Linear(self.hidden_size*8,29)\n",
    "        self.clf_2=nn.Linear(29,29)\n",
    "        self.clf_final=nn.Linear(58,class_num)\n",
    "  def forward(self, text_id,text_mask,features):\n",
    "    outputs = self.bert(input_ids=text_id,attention_mask=text_mask)\n",
    "    encoded_layers = outputs[2]\n",
    "    encoded_layer_1 = encoded_layers[-1].permute(1, 0, 2)\n",
    "    encoded_layer_2 = encoded_layers[-2].permute(1, 0, 2)\n",
    "    encoded_layer_3 = encoded_layers[-3].permute(1, 0, 2)\n",
    "    encoded_layer_4 = encoded_layers[-3].permute(1, 0, 2)\n",
    "    enc_hiddens, (last_hidden_1, last_cell) = self.LSTM_1(encoded_layer_1)\n",
    "    enc_hiddens, (last_hidden_2, last_cell) = self.LSTM_2(encoded_layer_2)\n",
    "    enc_hiddens, (last_hidden_3, last_cell) = self.LSTM_3(encoded_layer_3)\n",
    "    enc_hiddens, (last_hidden_4, last_cell) = self.LSTM_4(encoded_layer_4)\n",
    "    output_hidden = torch.cat((last_hidden_1[0], last_hidden_1[1],last_hidden_2[0], last_hidden_2[1],last_hidden_3[0], last_hidden_3[1],last_hidden_4[0], last_hidden_4[1]), dim=1)\n",
    "    output_hidden = F.dropout(output_hidden,0.2)\n",
    "    output_hidden = self.clf_1(output_hidden)\n",
    "    out_2=self.clf_2(features)\n",
    "    output = torch.cat((output_hidden,out_2), dim=1)\n",
    "    output=self.clf_final(output)\n",
    "    # output=torch.softmax(output, dim=1)\n",
    "    return output\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0158b939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from transformers import AutoModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def init_weights(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv2d') != -1 or classname.find('ConvTranspose2d') != -1:\n",
    "        nn.init.kaiming_uniform_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "class AttentionWithContext(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(AttentionWithContext, self).__init__()\n",
    "\n",
    "        self.attn = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.contx = nn.Linear(hidden_dim, 1, bias=False)\n",
    "        #self.apply(init_weights)\n",
    "    def forward(self, inp):\n",
    "        u = torch.tanh_(self.attn(inp))\n",
    "        a = F.softmax(self.contx(u))\n",
    "        s = (a * inp).sum(1)\n",
    "        return s\n",
    "\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 pretrained_path='aubmindlab/bert-base-arabert'):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "\n",
    "        \n",
    "        self.transformer = AutoModel.from_pretrained(pretrained_path, output_hidden_states=True)\n",
    "\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None):\n",
    "        outputs = self.transformer(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "            , output_hidden_states=True\n",
    "        )\n",
    "        #(output_last_layer, pooled_cls, (output_layers))\n",
    "        #output[0] (8, seqlen=64, 768) cls [8, 768] ( 12 (8, seqlen=64, 768))\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def output_num(self):\n",
    "        return self.transformer.config.hidden_size\n",
    "\n",
    "class ATTClassifier(nn.Module):\n",
    "    def __init__(self, in_feature, class_num=1, dropout_prob=0.2):\n",
    "        super(ATTClassifier, self).__init__()\n",
    "        self.attention = AttentionWithContext(in_feature)\n",
    "\n",
    "        self.Classifier = nn.Sequential(\n",
    "            nn.Linear(2 * in_feature, 512),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, class_num)\n",
    "        )\n",
    "\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        att = self.attention(x[0]) #(X[0] (bs, seqlenght, embedD) att = \\sum_i alpha_i x[0][i]\n",
    "\n",
    "        xx = torch.cat([att, x[1]], 1)\n",
    "\n",
    "        out = self.Classifier(xx)\n",
    "        return out\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        \n",
    "        self.supports_masking = True\n",
    "\n",
    "        self.bias = bias\n",
    "        self.feature_dim = feature_dim\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        \n",
    "        weight = torch.zeros(feature_dim, 1)\n",
    "        nn.init.kaiming_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "        \n",
    "        if bias:\n",
    "            self.b = nn.Parameter(torch.zeros(step_dim))\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        feature_dim = self.feature_dim \n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = torch.mm(\n",
    "            x.contiguous().view(-1, feature_dim), \n",
    "            self.weight\n",
    "        ).view(-1, step_dim)\n",
    "        \n",
    "        if self.bias:\n",
    "            eij = eij + self.b\n",
    "            \n",
    "        eij = torch.tanh(eij)\n",
    "        a = torch.exp(eij)\n",
    "        \n",
    "        if mask is not None:\n",
    "            a = a * mask\n",
    "\n",
    "        a = a / (torch.sum(a, 1, keepdim=True) + 1e-10)\n",
    "\n",
    "        weighted_input = x * torch.unsqueeze(a, -1)\n",
    "        return torch.sum(weighted_input, 1)\n",
    "\n",
    "class WeightedLayerPooling(nn.Module):\n",
    "    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights=None):\n",
    "        super(WeightedLayerPooling, self).__init__()\n",
    "        self.layer_start = layer_start\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.layer_weights = layer_weights if layer_weights is not None \\\n",
    "            else nn.Parameter(\n",
    "            torch.tensor([1] * (num_hidden_layers + 1 - layer_start), dtype=torch.float)\n",
    "        )\n",
    "\n",
    "    def forward(self, all_hidden_states):\n",
    "        all_layer_embedding = all_hidden_states[self.layer_start:, :, :, :]\n",
    "        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n",
    "        weighted_average = (weight_factor * all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n",
    "        return weighted_average\n",
    "class NADIModelMTL(nn.Module):\n",
    "  def __init__(self, pretrained_path='aubmindlab/bert-base-arabert',in_feature=768, class_num=3, dropout_prob=0.2):\n",
    "        super(NADIModelMTL, self).__init__()\n",
    "        self.base_model=TransformerLayer(pretrained_path).to('cuda')\n",
    "        self.config = AutoConfig.from_pretrained(pretrained_path)\n",
    "        self.weighted_pooler = WeightedLayerPooling(num_hidden_layers=self.config.num_hidden_layers, layer_start=4)\n",
    "        self.att1=Attention(768, 512).to('cuda')\n",
    "        self.classifier1=nn.Linear(768, 18)\n",
    "        self.att2=Attention(768, 512).to('cuda')\n",
    "        self.classifier2=nn.Linear(768, 3)\n",
    "  def forward(self, ids,mask):\n",
    "    output=self.base_model(ids,mask)\n",
    "    all_hidden_states = torch.stack(output.hidden_states)\n",
    "    weighted_pooling_embeddings = (self.weighted_pooler(all_hidden_states)) # For WeightedLayerPooling\n",
    "    # weighted_pooling_embeddings = weighted_pooling_embeddings[:, 0]\n",
    "    # print(weighted_pooling_embeddings.shape)\n",
    "    output1=self.att1(weighted_pooling_embeddings)\n",
    "    output1=self.classifier1(output1)\n",
    "    \n",
    "    output2=self.att2(weighted_pooling_embeddings)\n",
    "    output2=self.classifier2(output2)\n",
    "    # output=torch.softmax(output, dim=1)\n",
    "    return output1, output2\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97abd9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from transformers import AutoModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from typing import (\n",
    "    List\n",
    ")\n",
    "import torch as T\n",
    "import math\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from transformers.models.bert.modeling_bert import BertSelfOutput, BertOutput, BertAttention, BertLayer\n",
    "\n",
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "from transformers.models.bert.modeling_bert import BertSelfOutput, BertOutput, BertAttention, BertLayer\n",
    "\n",
    "\n",
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "from transformers.models.bert.modeling_bert import BertSelfOutput, BertOutput, BertAttention, BertLayer\n",
    "\n",
    "def random_mask_tokens(input_ids, atttention_mask, masking_percentage, mask_id, device):\n",
    "    index_bool = torch.ones(input_ids.shape[0],input_ids.shape[1])*masking_percentage\n",
    "    index_bool = torch.where((input_ids != mask_id) * (input_ids != 1) * (input_ids != -100) * (input_ids != 2) * (input_ids != 3) * (input_ids != 0), index_bool.float().to(device), torch.zeros(input_ids.shape[0],input_ids.shape[1]).to(device))\n",
    "    index_bool = index_bool.to(device) \n",
    "    index_bool = torch.mul(index_bool, atttention_mask) # To remove the ones that are masked from consideration\n",
    "    index_bool = torch.bernoulli(index_bool).bool()\n",
    "    input_ids[index_bool] = mask_id\n",
    "    return input_ids\n",
    "# Taking each huggingface layer and overriding what should be overriden\n",
    "\n",
    "class BertSelfOutput_w_adapters(BertSelfOutput):\n",
    "    def __init__(self, config, bottleneck_dim, current_adapter_to_train, no_total_adapters, stage_2_training, use_adapt_after_fusion):\n",
    "        super(BertSelfOutput_w_adapters, self).__init__(config)\n",
    "        self.adapter_layer = AdapterFusionModule(config, bottleneck_dim, current_adapter_to_train, no_total_adapters, stage_2_training, use_adapt_after_fusion)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = hidden_states + self.adapter_layer(hidden_states) # Residual/Skip-Connection\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class BertOutput_w_adapters(BertOutput):\n",
    "    def __init__(self, config, bottleneck_dim, current_adapter_to_train, no_total_adapters, stage_2_training, use_adapt_after_fusion):\n",
    "        super(BertOutput_w_adapters, self).__init__(config)\n",
    "        self.adapter_layer = AdapterFusionModule(config, bottleneck_dim, current_adapter_to_train, no_total_adapters, stage_2_training, use_adapt_after_fusion)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = hidden_states + self.adapter_layer(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class AdapterAttention(BertAttention):\n",
    "    def __init__(self, config, bottleneck_dim, current_adapter_to_train, no_total_adapters, stage_2_training, use_adapt_after_fusion):\n",
    "        super(AdapterAttention, self).__init__(config)\n",
    "        self.output = BertSelfOutput_w_adapters(config, bottleneck_dim, current_adapter_to_train, no_total_adapters, stage_2_training, use_adapt_after_fusion) # Override with new SelfOut\n",
    "\n",
    "\n",
    "class BertLayer_w_Adapters(BertLayer):\n",
    "    def __init__(self, config, bottleneck_dim, current_adapter_to_train, no_total_adapters, stage_2_training, use_adapt_after_fusion):\n",
    "        super(BertLayer_w_Adapters, self).__init__(config)\n",
    "        self.attention = AdapterAttention(config, bottleneck_dim, current_adapter_to_train, no_total_adapters, stage_2_training, use_adapt_after_fusion)\n",
    "        # self.attention = BertAttention(config)\n",
    "        self.is_decoder = config.is_decoder\n",
    "        if self.is_decoder:\n",
    "            self.crossattention = AdapterAttention(config, bottleneck_dim, current_adapter_to_train, no_total_adapters, stage_2_training, use_adapt_after_fusion)\n",
    "        self.output = BertOutput_w_adapters(config, bottleneck_dim, current_adapter_to_train, no_total_adapters, stage_2_training, use_adapt_after_fusion)\n",
    "\n",
    "class AdapterModule(nn.Module):\n",
    "    def __init__(self, hidden_size, bottleneck_dim):\n",
    "        super(AdapterModule, self).__init__()\n",
    "        self.down_project = nn.Linear(hidden_size, bottleneck_dim)\n",
    "        self.up_project = nn.Linear(bottleneck_dim, hidden_size)\n",
    "        self.non_linearity = nn.GELU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        down_projection = self.down_project(x)\n",
    "        non_linearity = self.non_linearity(down_projection)\n",
    "        up_projection = self.up_project(non_linearity)\n",
    "        #up_projection = self.dropout(up_projection)\n",
    "\n",
    "        return up_projection\n",
    "\n",
    "## Inspired from fusion implementation in adapter-transformers repo https://github.com/Adapter-Hub/adapter-transformers\n",
    "## From Class BERT Fusion implemented in: https://github.com/Adapter-Hub/adapter-transformers/blob/master/src/transformers/adapter_modeling.py\n",
    "\n",
    "class FusionAttn(nn.Module):\n",
    "    def __init__(self, config, bottleneck_dim, use_adapt_after_fusion):\n",
    "        super(FusionAttn, self).__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.dense = nn.Linear(config.hidden_size, 1)\n",
    "        self.query = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.key = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.value = nn.Linear(config.hidden_size, config.hidden_size, bias=False)\n",
    "        self.T = 1\n",
    "        self.reduction = self.T / 1000.0\n",
    "        self.adapter_after_fusion = AdapterModule(config.hidden_size, bottleneck_dim)\n",
    "        self.use_adapter_after_fusion = use_adapt_after_fusion\n",
    "        \n",
    "    def forward(self, hidden_states_before, adapters_output, attention_mask=None):\n",
    "        key = adapters_output.permute(2, 1, 0, 3)\n",
    "        value = adapters_output.permute(2, 1, 0, 3)\n",
    "        query = hidden_states_before.permute(1, 0, 2)\n",
    "        residual = hidden_states_before.permute(1, 0, 2)\n",
    "    \n",
    "        value += residual[:, :, None, :].repeat(1, 1, value.size(2), 1)\n",
    "\n",
    "        query_layer = self.query(query)\n",
    "\n",
    "        key_layer = self.key(key)\n",
    "        value_layer = self.value(value)\n",
    "\n",
    "        attention_scores = torch.squeeze(torch.matmul(query_layer.unsqueeze(2), key_layer.transpose(-2, -1)), dim=2)\n",
    "\n",
    "        attention_scores = self.dropout(attention_scores)\n",
    "\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores / self.T)\n",
    "        self.T = max(self.T - self.reduction, 1.0)\n",
    "\n",
    "        context_layer = torch.squeeze(torch.matmul(attention_probs.unsqueeze(2), value_layer), dim=2)\n",
    "\n",
    "        context_layer = context_layer.permute(1, 0, 2)\n",
    "        if self.use_adapter_after_fusion: \n",
    "            context_layer = self.adapter_after_fusion(context_layer)\n",
    "        return context_layer\n",
    "\n",
    "\n",
    "class AdapterFusionModule(nn.Module):\n",
    "    def __init__(self, config, bottleneck_dim, current_adapter_to_train, no_total_adapters, stage_2_training, use_adapt_after_fusion):\n",
    "        super(AdapterFusionModule, self).__init__()\n",
    "        self.stage_2_training = stage_2_training\n",
    "        self.adapter_to_train_index = current_adapter_to_train\n",
    "        self.number_of_tasks = no_total_adapters\n",
    "        self.list_of_adapter_modules = nn.ModuleList([AdapterModule(config.hidden_size, 64) for _ in range(self.number_of_tasks)]) \n",
    "        self.adapter_fusion_attention_layer = FusionAttn(config, 64, use_adapt_after_fusion)\n",
    "     \n",
    "        if self.stage_2_training:\n",
    "            for p in self.list_of_adapter_modules.named_parameters():\n",
    "                p[1].requires_grad = False\n",
    "        else:\n",
    "            for p in self.adapter_fusion_attention_layer.named_parameters():\n",
    "                p[1].requires_grad = False\n",
    "            for adapter_index in range(self.number_of_tasks):\n",
    "                if adapter_index != self.adapter_to_train_index:\n",
    "                    for p in self.list_of_adapter_modules[adapter_index].named_parameters():\n",
    "                        p[1].requires_grad = False\n",
    "\n",
    "    \n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        if not self.stage_2_training:\n",
    "            return self.list_of_adapter_modules[self.adapter_to_train_index](hidden_states)\n",
    "        else:\n",
    "            adapters_output = []\n",
    "            for adapter_layer in self.list_of_adapter_modules:\n",
    "                output_adapter_layer = adapter_layer(hidden_states)\n",
    "                adapters_output.append(output_adapter_layer)\n",
    "            adapters_output = torch.stack(adapters_output)\n",
    "\n",
    "            final_hidden_states = self.adapter_fusion_attention_layer(hidden_states, adapters_output)\n",
    "            return final_hidden_states\n",
    "        \n",
    "def split_dim(x: T.Tensor, n: int, *, dim: int=-1):\n",
    "    assert -x.ndim <= dim < x.ndim\n",
    "    assert x.shape[dim] % n == 0\n",
    "    if dim < 0:\n",
    "        dim = dim + x.ndim\n",
    "    sh = x.shape[:dim] + (x.shape[dim] // n, n) + x.shape[dim+1:]\n",
    "    z = x.view(sh)\n",
    "    return z\n",
    "\n",
    "class EnhancedLinear(nn.Module):\n",
    "    '''\"Stop thinking with your head. -- SMerity'''\n",
    "    def __init__(\n",
    "            self,\n",
    "            Din: int,\n",
    "            Dout: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.Din = Din\n",
    "        self.Dout = Dout\n",
    "    \n",
    "        self.static = nn.Parameter(T.ones(self.Dout), requires_grad=True).to(\"cuda:0\")\n",
    "        self.linear = nn.Linear(self.Din, self.Dout * 2).to(\"cuda:0\")\n",
    "\n",
    "    def forward(self, X: T.Tensor) -> T.Tensor:\n",
    "        mag, sgn = self.linear(X.to(\"cuda:0\")).chunk(2, dim=-1)\n",
    "        Z = self.static * mag.sigmoid() * sgn.tanh()\n",
    "        return Z\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self,  config):\n",
    "        super().__init__()\n",
    "\n",
    "#         self.args = args\n",
    "        self.config = config\n",
    "        self.use_common_transform = 384\n",
    "        self.nb_layers = config.num_hidden_layers\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.T = 1\n",
    "        # self.reduction = self.T / 1000.0\n",
    "        self.reduction = 0.0\n",
    "        self.use_adapter = False\n",
    "        self.positional_keys_mode = 'sinosoid'\n",
    "        self.do_debug_shapes = False\n",
    "\n",
    "        self.build_()\n",
    "\n",
    "    def build_key_transform_(self):\n",
    "#         args = self.args\n",
    "        config = self.config\n",
    "        if self.positional_keys_mode:\n",
    "            key_size = self.nb_layers * 2\n",
    "            if self.positional_keys_mode == 'random':\n",
    "                self.StaticKeys = [\n",
    "                    T.rand(key_size).to(\"cuda:0\") # .cuda()\n",
    "                    for _ in range(self.nb_layers)\n",
    "                ]\n",
    "            elif self.positional_keys_mode == 'sinosoid':\n",
    "                self.StaticKeys = []\n",
    "                di = T.arange(key_size)\n",
    "                di = (di // 2).type(T.float32) / key_size\n",
    "                di = T.div(1.0, T.pow(100, di))\n",
    "                for ilayer in range(self.nb_layers):\n",
    "                    enc = di * ilayer\n",
    "                    enc[0::2].sin_()\n",
    "                    enc[1::2].cos_()\n",
    "                    self.StaticKeys.append(enc) # .cuda()\n",
    "            else:\n",
    "                raise KeyError(f\"Unknown positional key mode: {self.positional_keys_mode}.\")\n",
    "            shared_key_transform = EnhancedLinear(key_size, self.hidden_size).to(\"cuda:0\")\n",
    "            self.key_transforms = nn.ModuleList([\n",
    "                shared_key_transform\n",
    "                for _ in range(self.nb_layers)\n",
    "            ])\n",
    "        else:\n",
    "            self.key_transforms = nn.ModuleList([\n",
    "                nn.Linear(self.hidden_size, self.hidden_size)\n",
    "                for _ in range(self.nb_layers)\n",
    "            ])\n",
    "\n",
    "    def build_(self):\n",
    "#         args = self.args\n",
    "        config = self.config\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.query   = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        if self.use_common_transform:\n",
    "            self.key_c   = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "            self.value_c = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "        self.build_key_transform_()\n",
    "        self.value_transforms   = nn.ModuleList([\n",
    "            nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "            for _ in range(self.nb_layers)\n",
    "        ])\n",
    "        self.adapter = AdapterModule(self.hidden_size, 384)\n",
    "        self.values_lnorm = nn.LayerNorm(self.hidden_size)\n",
    "\n",
    "    def debug_shapes(self, tensor, name: str):\n",
    "        if self.do_debug_shapes:\n",
    "            print(f'{name}.shape=={tensor.shape}')\n",
    "\n",
    "    def Tquery(self, query: T.Tensor) -> T.Tensor:\n",
    "        #^ query: [batch, dim]\n",
    "        #^ => [b, Qdim]\n",
    "        return self.query(query)\n",
    "\n",
    "    def Tkeys(self, Xs: List[T.Tensor]) -> T.Tensor:\n",
    "        if self.positional_keys_mode:\n",
    "            Xs = self.StaticKeys\n",
    "            #^ Xs: [layer][KeySize]\n",
    "            Xs = [xs.unsqueeze(0) for xs in Xs]\n",
    "            #^ Xs: [layer][1, KeySize]\n",
    "        Z = [\n",
    "            self.key_c(tkey(xs))\n",
    "            if self.use_common_transform\n",
    "            else tkey(xs)\n",
    "            for tkey, xs in zip(self.key_transforms, Xs)\n",
    "        ]\n",
    "        return T.stack(Z, dim=1)\n",
    "        #^ => [b|1, layer, Qdim]\n",
    "\n",
    "    def Tvalues(self, Xs: List[T.Tensor]) -> T.Tensor:\n",
    "        #^ => [b, Vdim, layer]\n",
    "        if self.use_common_transform:\n",
    "            Z = [\n",
    "                self.value_c(tlvalue(xs))\n",
    "                for tlvalue, xs in zip(self.value_transforms, Xs)\n",
    "            ]\n",
    "        else:\n",
    "            Z = [\n",
    "                tlvalue(xs)\n",
    "                for tlvalue, xs in zip(self.value_transforms, Xs)\n",
    "            ]\n",
    "        return T.stack(Z, dim=2)\n",
    "\n",
    "    def forward(self, Xs: List[T.Tensor], Q: T.Tensor) -> T.Tensor:\n",
    "        #^ => [b, Vd] or [b, AdapterD]\n",
    "        #^ Xs: [layer][batch, dim]\n",
    "        #^ Q: [batch, dim]\n",
    "        Xs = [self.dropout(x) for x in Xs]\n",
    "\n",
    "        query = self.Tquery(Q)\n",
    "        #^ query: [batch, Qdim]\n",
    "        keys = self.Tkeys(Xs)\n",
    "        #^ keys: [batch, layer, Qdim]\n",
    "        values = self.Tvalues(Xs)\n",
    "        #^ values: [batch, Vdim, layer]\n",
    "        residual = Xs[-1]\n",
    "        #^ residual: [batch, Vdim]\n",
    "\n",
    "        # values += residual.unsqueeze(2)\n",
    "        #^ values: [batch, Vdim, layer]\n",
    "\n",
    "        # values = self.dropout(values)\n",
    "        values = self.values_lnorm(values.transpose(-1, -2)).transpose(-1, -2)\n",
    "        #^ => [batch, layer, Vdim] => [batch, norm(Vdim), layer]\n",
    "\n",
    "        # query_layer = self.query(query)\n",
    "        # key_layer = self.key_transforms(key)\n",
    "        # value_layer = self.value_transforms(value)\n",
    "\n",
    "        attention_logits = T.matmul(query.unsqueeze(1), keys.transpose(-1, -2)).squeeze(dim=1)\n",
    "        #^ [b, 1, Qd] matmul [b, (Qd, L)] => [b, L]\n",
    "\n",
    "        attention_logits = self.dropout(attention_logits)\n",
    "\n",
    "        attention_probs = F.softmax(attention_logits / math.sqrt(self.hidden_size), dim=-1)\n",
    "        # attention_probs = nn.Softmax(dim=-1)(attention_logits / math.sqrt(self.hidden_size) / self.T)\n",
    "        # self.T = max(self.T - self.reduction, 1.0)\n",
    "\n",
    "        context_layer = T.matmul(attention_probs.unsqueeze(1), values.transpose(-1, -2)).squeeze(dim=1)\n",
    "        #^ [b, 1, L] matmul [b, (L, Vd)] => [b, Vd]\n",
    "\n",
    "        # context_layer += self.value_transforms[-1](residual)\n",
    "        context_layer += residual\n",
    "\n",
    "        if self.use_adapter: \n",
    "            context_layer = self.adapter(context_layer)\n",
    "        return context_layer\n",
    " \n",
    "    \n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, input_dim, num_labels, dropout_rate=0., depth=1):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.depth = depth\n",
    "        assert depth in (1, 2)\n",
    "        if depth == 1:\n",
    "            self.linear = nn.Linear(input_dim, num_labels)  # Simple Head for Now\n",
    "        else:\n",
    "            self.linear = nn.Sequential(\n",
    "                nn.Linear(input_dim, input_dim // 2),\n",
    "                nn.ReLU(),\n",
    "                self.dropout,\n",
    "                nn.Linear(input_dim // 2, num_labels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        return self.linear(x)\n",
    "class ArabicDialectBERT(BertPreTrainedModel):\n",
    "    def __init__(self,config,model_path='aubmindlab/bert-base-arabert',num_labels=3):\n",
    "        super(ArabicDialectBERT, self).__init__(config)\n",
    "        \n",
    "        self.num_labels = num_labels\n",
    "        self.config = AutoConfig.from_pretrained(model_path)\n",
    "        self.bert = AutoModel.from_pretrained(model_path, output_hidden_states=True)\n",
    "        self.masking_perc = 0\n",
    "        self.tokenizer= AutoTokenizer.from_pretrained('UBC-NLP/MARBERTv2')\n",
    "        self.mask_id =  self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "        self.device_name = \"cuda:0\"\n",
    "#         if args[\"use_adapters\"]:\n",
    "#             if args[\"adapter_type\"] == \"Fusion\":\n",
    "        self.bert.encoder.layer = nn.ModuleList([\n",
    "            BertLayer_w_Adapters(config,\n",
    "                384, 6, 21, True, True)\n",
    "                for _ in range(config.num_hidden_layers)])\n",
    "                # self.bert.encoder.layer = nn.ModuleList([BertLayer(config) for _ in range(11)] + [BertLayer_w_Adapters(config, args[\"bottleneck_dim\"], args[\"current_adapter_to_train\"], args[\"no_total_adapters\"], args[\"stage_2_training\"], args[\"use_adapt_after_fusion\"]) for _ in range(1)])\n",
    "#             elif args[\"adapter_type\"] == \"plain_adapter\":\n",
    "#                 self.bert.encoder.layer = nn.ModuleList([\n",
    "#                     BertLayer_w_PlainAdapters(config,\n",
    "#                         args[\"bottleneck_dim\"], args[\"current_adapter_to_train\"], args[\"no_total_adapters\"], args[\"stage_2_training\"], args[\"use_adapt_after_fusion\"])\n",
    "#                         for _ in range(config.num_hidden_layers)])\n",
    "                # self.bert.encoder.layer = nn.ModuleList([BertLayer(config) for _ in range(10)] + [BertLayer_w_PlainAdapters(config, args[\"bottleneck_dim\"], args[\"current_adapter_to_train\"], args[\"no_total_adapters\"], args[\"stage_2_training\"], args[\"use_adapt_after_fusion\"]) for _ in range(2)])\n",
    "        for param in self.bert.encoder.layer.named_parameters():\n",
    "            if \"adapter_layer\" not in param[0]: #or \"list_of_adapter_modules\" in param[0]:\n",
    "                param[1].requires_grad = False\n",
    "            else:\n",
    "                print(param[0])\n",
    "            # Freeze all except adapters and head\n",
    "\n",
    "#         if args[\"use_vert_att\"]:\n",
    "        self.attend_vertical = SelfAttention(config).to(\"cuda:0\")\n",
    "\n",
    "        # self.loss_function = nn.CrossEntropyLoss(weight=torch.tensor(self.args[\"cls_weights\"]))\n",
    "#         self.loss_function = nn.CrossEntropyLoss()\n",
    "        self.classif_head = ClassificationHead(config.hidden_size, self.num_labels,0.3).to(\"cuda:0\")\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        if self.train:\n",
    "            input_ids = random_mask_tokens(input_ids, attention_mask, self.masking_perc, self.mask_id, self.device_name)\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            output_hidden_states=True,\n",
    "        )  # sequence_output, pooled_output, (hidden_states), (attentions)\n",
    "        # sequence_output = outputs[0]  # Not needed for now\n",
    "        pooled_output = outputs[1]  # [CLS]\n",
    "        \n",
    "#         if self.args['use_vert_att']:\n",
    "        layer_cls = [layer[:,0,:] for layer in outputs[2]]\n",
    "        pooled_output = self.attend_vertical(Xs=layer_cls, Q=pooled_output)\n",
    "            # pooled_output = self.bert.pooler(new_feats)\n",
    "\n",
    "        logits = self.classif_head(pooled_output)\n",
    "\n",
    "#         total_loss = 0\n",
    "#         # 1. Intent Softmax\n",
    "#         if class_label_ids is not None:\n",
    "#             if self.num_labels == 1:\n",
    "#                 loss_function = nn.MSELoss()\n",
    "#                 temp_loss = loss_function(logits.view(-1), class_label_ids.view(-1))\n",
    "#             else:\n",
    "#                 temp_loss = self.loss_function(logits.view(-1, self.num_labels), class_label_ids.view(-1))\n",
    "#             total_loss += temp_loss  \n",
    "\n",
    "#         outputs = ((logits, ),) + outputs[2:]  # add hidden states and attention if they are here\n",
    "\n",
    "#         outputs = (total_loss,) + outputs\n",
    "\n",
    "        return logits  # (loss), logits, (hidden_states), (attentions) # Logits is a tuple of intent and slot logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "894196a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from transformers import AutoModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def init_weights(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv2d') != -1 or classname.find('ConvTranspose2d') != -1:\n",
    "        nn.init.kaiming_uniform_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "class AttentionWithContext(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(AttentionWithContext, self).__init__()\n",
    "\n",
    "        self.attn = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.contx = nn.Linear(hidden_dim, 1, bias=False)\n",
    "        #self.apply(init_weights)\n",
    "    def forward(self, inp):\n",
    "        u = torch.tanh_(self.attn(inp))\n",
    "        a = F.softmax(self.contx(u))\n",
    "        s = (a * inp).sum(1)\n",
    "        return s\n",
    "\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 pretrained_path='aubmindlab/bert-base-arabert'):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "\n",
    "        \n",
    "        self.transformer = AutoModel.from_pretrained(pretrained_path, output_hidden_states=True)\n",
    "\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None):\n",
    "        outputs = self.transformer(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "            , output_hidden_states=True\n",
    "        )\n",
    "        #(output_last_layer, pooled_cls, (output_layers))\n",
    "        #output[0] (8, seqlen=64, 768) cls [8, 768] ( 12 (8, seqlen=64, 768))\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def output_num(self):\n",
    "        return self.transformer.config.hidden_size\n",
    "\n",
    "class ATTClassifier(nn.Module):\n",
    "    def __init__(self, in_feature, class_num=1, dropout_prob=0.2):\n",
    "        super(ATTClassifier, self).__init__()\n",
    "        self.attention = AttentionWithContext(in_feature)\n",
    "\n",
    "        self.Classifier = nn.Sequential(\n",
    "            nn.Linear(2 * in_feature, 512),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, class_num)\n",
    "        )\n",
    "\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        att = self.attention(x[0]) #(X[0] (bs, seqlenght, embedD) att = \\sum_i alpha_i x[0][i]\n",
    "\n",
    "        xx = torch.cat([att, x[1]], 1)\n",
    "\n",
    "        out = self.Classifier(xx)\n",
    "        return out\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        \n",
    "        self.supports_masking = True\n",
    "\n",
    "        self.bias = bias\n",
    "        self.feature_dim = feature_dim\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        \n",
    "        weight = torch.zeros(feature_dim, 1)\n",
    "        nn.init.kaiming_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "        \n",
    "        if bias:\n",
    "            self.b = nn.Parameter(torch.zeros(step_dim))\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        feature_dim = self.feature_dim \n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = torch.mm(\n",
    "            x.contiguous().view(-1, feature_dim), \n",
    "            self.weight\n",
    "        ).view(-1, step_dim)\n",
    "        \n",
    "        if self.bias:\n",
    "            eij = eij + self.b\n",
    "            \n",
    "        eij = torch.tanh(eij)\n",
    "        a = torch.exp(eij)\n",
    "        \n",
    "        if mask is not None:\n",
    "            a = a * mask\n",
    "\n",
    "        a = a / (torch.sum(a, 1, keepdim=True) + 1e-10)\n",
    "\n",
    "        weighted_input = x * torch.unsqueeze(a, -1)\n",
    "        return torch.sum(weighted_input, 1)\n",
    "\n",
    "class WeightedLayerPooling(nn.Module):\n",
    "    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights=None):\n",
    "        super(WeightedLayerPooling, self).__init__()\n",
    "        self.layer_start = layer_start\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.layer_weights = layer_weights if layer_weights is not None \\\n",
    "            else nn.Parameter(\n",
    "            torch.tensor([1] * (num_hidden_layers + 1 - layer_start), dtype=torch.float)\n",
    "        )\n",
    "\n",
    "    def forward(self, all_hidden_states):\n",
    "        all_layer_embedding = all_hidden_states[self.layer_start:, :, :, :]\n",
    "        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n",
    "        weighted_average = (weight_factor * all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n",
    "        return weighted_average\n",
    "class NADIModelArabert(nn.Module):\n",
    "  def __init__(self, pretrained_path='aubmindlab/bert-base-arabert',in_feature=768, class_num=3, dropout_prob=0.2):\n",
    "        super(NADIModelArabert, self).__init__()\n",
    "        self.base_model=TransformerLayer(pretrained_path).to('cuda')\n",
    "        self.config = AutoConfig.from_pretrained(pretrained_path)\n",
    "        self.weighted_pooler = WeightedLayerPooling(num_hidden_layers=self.config.num_hidden_layers, layer_start=4)\n",
    "        self.att=Attention(768, 512).to('cuda')\n",
    "        self.classifier=nn.Linear(768, class_num)\n",
    "  def forward(self, ids,mask):\n",
    "    output=self.base_model(ids,mask)\n",
    "    all_hidden_states = torch.stack(output.hidden_states)\n",
    "    weighted_pooling_embeddings = (self.weighted_pooler(all_hidden_states)) # For WeightedLayerPooling\n",
    "    # weighted_pooling_embeddings = weighted_pooling_embeddings[:, 0]\n",
    "    # print(weighted_pooling_embeddings.shape)\n",
    "    output=self.att(weighted_pooling_embeddings)\n",
    "    output=self.classifier(output)\n",
    "    # output=torch.softmax(output, dim=1)\n",
    "    return output\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93c7057f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the test value result\n",
    "def inference_fn_MTL_task_1(test_loader, model, device):\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    tk0 = tqdm(test_loader, total=len(test_loader))\n",
    "    for data in tk0:\n",
    "        ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "        mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "        with torch.no_grad():\n",
    "            y_preds,_ = model(ids, mask)\n",
    "        y_preds = (F.softmax(y_preds).to('cpu').numpy())\n",
    "        # y_preds = y_preds.to('cpu').numpy()\n",
    "        preds.append(y_preds)\n",
    "    preds = np.concatenate(preds)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7e3442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the test value result\n",
    "def inference_fn_MTL_task_2(test_loader, model, device):\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    tk0 = tqdm(test_loader, total=len(test_loader))\n",
    "    for data in tk0:\n",
    "        ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "        mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "        with torch.no_grad():\n",
    "            _,y_preds = model(ids, mask)\n",
    "        y_preds = (F.softmax(y_preds).to('cpu').numpy())\n",
    "        # y_preds = y_preds.to('cpu').numpy()\n",
    "        preds.append(y_preds)\n",
    "    preds = np.concatenate(preds)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0e4743f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the test value result\n",
    "def inference_fn(test_loader, model, device):\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    tk0 = tqdm(test_loader, total=len(test_loader))\n",
    "    for data in tk0:\n",
    "        ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "        mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(ids, mask)\n",
    "        y_preds = (F.softmax(y_preds).to('cpu').numpy())\n",
    "        # y_preds = y_preds.to('cpu').numpy()\n",
    "        preds.append(y_preds)\n",
    "    preds = np.concatenate(preds)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a3a5a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the test value result\n",
    "def inference_fn_marbert(test_loader, model, device):\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    tk0 = tqdm(test_loader, total=len(test_loader))\n",
    "    for data in tk0:\n",
    "        ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "        mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(ids, mask,token_type_ids)\n",
    "        y_preds = (F.softmax(y_preds).to('cpu').numpy())\n",
    "        # y_preds = y_preds.to('cpu').numpy()\n",
    "        preds.append(y_preds)\n",
    "    preds = np.concatenate(preds)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "270ff163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the test value result\n",
    "def inference_fn_marbert_features(test_loader, model, device):\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    tk0 = tqdm(test_loader, total=len(test_loader))\n",
    "    for data in tk0:\n",
    "        ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "        mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        fetures=data['features'].to(device, dtype = torch.float)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(ids, mask,fetures)\n",
    "        y_preds = (F.softmax(y_preds).to('cpu').numpy())\n",
    "        # y_preds = y_preds.to('cpu').numpy()\n",
    "        preds.append(y_preds)\n",
    "    preds = np.concatenate(preds)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3b2af4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from transformers import AutoModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def init_weights(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv2d') != -1 or classname.find('ConvTranspose2d') != -1:\n",
    "        nn.init.kaiming_uniform_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "class AttentionWithContext(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(AttentionWithContext, self).__init__()\n",
    "\n",
    "        self.attn = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.contx = nn.Linear(hidden_dim, 1, bias=False)\n",
    "        #self.apply(init_weights)\n",
    "    def forward(self, inp):\n",
    "        u = torch.tanh_(self.attn(inp))\n",
    "        a = F.softmax(self.contx(u))\n",
    "        s = (a * inp).sum(1)\n",
    "        return s\n",
    "\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 pretrained_path='aubmindlab/bert-base-arabert'):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "\n",
    "        \n",
    "        self.transformer = AutoModel.from_pretrained(pretrained_path, output_hidden_states=True)\n",
    "\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None):\n",
    "        outputs = self.transformer(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        #(output_last_layer, pooled_cls, (output_layers))\n",
    "        #output[0] (8, seqlen=64, 768) cls [8, 768] ( 12 (8, seqlen=64, 768))\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def output_num(self):\n",
    "        return self.transformer.config.hidden_size\n",
    "\n",
    "class ATTClassifier(nn.Module):\n",
    "    def __init__(self, in_feature, class_num=1, dropout_prob=0.2):\n",
    "        super(ATTClassifier, self).__init__()\n",
    "        self.attention = AttentionWithContext(in_feature)\n",
    "\n",
    "        self.Classifier = nn.Sequential(\n",
    "            nn.Linear(2 * in_feature, 512),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, class_num)\n",
    "        )\n",
    "\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        att = self.attention(x[0]) #(X[0] (bs, seqlenght, embedD) att = \\sum_i alpha_i x[0][i]\n",
    "\n",
    "        xx = torch.cat([att, x[1]], 1)\n",
    "\n",
    "        out = self.Classifier(xx)\n",
    "        return out\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        \n",
    "        self.supports_masking = True\n",
    "\n",
    "        self.bias = bias\n",
    "        self.feature_dim = feature_dim\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        \n",
    "        weight = torch.zeros(feature_dim, 1)\n",
    "        nn.init.kaiming_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "        \n",
    "        if bias:\n",
    "            self.b = nn.Parameter(torch.zeros(step_dim))\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        feature_dim = self.feature_dim \n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = torch.mm(\n",
    "            x.contiguous().view(-1, feature_dim), \n",
    "            self.weight\n",
    "        ).view(-1, step_dim)\n",
    "        \n",
    "        if self.bias:\n",
    "            eij = eij + self.b\n",
    "            \n",
    "        eij = torch.tanh(eij)\n",
    "        a = torch.exp(eij)\n",
    "        \n",
    "        if mask is not None:\n",
    "            a = a * mask\n",
    "\n",
    "        a = a / (torch.sum(a, 1, keepdim=True) + 1e-10)\n",
    "\n",
    "        weighted_input = x * torch.unsqueeze(a, -1)\n",
    "        return torch.sum(weighted_input, 1)\n",
    "\n",
    "class NADIModelElectra(nn.Module):\n",
    "  def __init__(self, pretrained_path='aubmindlab/bert-base-arabert',in_feature=768, class_num=3, dropout_prob=0.2):\n",
    "        super(NADIModelElectra, self).__init__()\n",
    "        self.base_model=TransformerLayer(pretrained_path).to('cuda')\n",
    "        self.att=Attention(self.base_model.output_num(), 512).to('cuda')\n",
    "        self.classifier=nn.Linear(self.base_model.output_num(), class_num)\n",
    "  def forward(self, ids,mask):\n",
    "    output=self.base_model(ids,mask)\n",
    "    output=self.att(output.last_hidden_state)\n",
    "    output=self.classifier(output)\n",
    "    # output=torch.softmax(output, dim=1)\n",
    "    return output\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd9bc256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class that expects a prompted input from the BERTPromptDataset\n",
    "# Takes the input, forward propagates it through BERT and concatenates the output at the [MASK] and [CLS] token to get a representation of the text\n",
    "# This is then passed to a linear head to perform binary classification for how relevant it is\n",
    "class BERTPrompt(torch.nn.Module):\n",
    "    def __init__(self, bert, tokenizer):\n",
    "        super().__init__()\n",
    "        self.bert = bert.cuda()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.linear = torch.nn.Linear(768*2, 3).cuda()\n",
    "        self.act = torch.nn.Softmax()\n",
    "        \n",
    "    # input_dict is obtained through indexing the dataset e.g. dataset[0:10]\n",
    "    def forward(self, input_ids,attention_mask,mask_pos ):\n",
    "        output = self.bert(input_ids,attention_mask)[0] # output is of shape [10, 256, 768]\n",
    "        cls_out = output[:, 0, :]\n",
    "        mask_out = output[torch.arange(cls_out.shape[0]), mask_pos, :] # indexing like [[0, 1], [13, 14]] will select items [[0, 13], [1, 14]]\n",
    "        \n",
    "        representation = torch.cat([cls_out, mask_out], dim=1)\n",
    "        logit = self.linear(representation)\n",
    "        return logit\n",
    "    \n",
    "#     # return the logit with sigmoid activation applied\n",
    "#     def predict(self, input_dict):\n",
    "#         return self.forward(input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e64ab91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dataset class for prompted BERT fine tuning\n",
    "# The dataset takes in query and passage, and then construct a training sample as:  <prompt> + [MASK] + <text>, returning the position of the mask\n",
    "# It also performs padding and stores the labels\n",
    "class BERTPromptDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset,  tokenizer, prompt=\"   \", max_length=512):\n",
    "        # Construct the input sentence\n",
    "        input_sentences = [\"{} {} {}\".format(prompt, tokenizer.mask_token, text) for _, (text) in dataset.iterrows()]\n",
    "#         print(input_sentences)\n",
    "        \n",
    "        # Encode and store\n",
    "        encodings_dict = tokenizer.batch_encode_plus(input_sentences, truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "        self.input_ids = encodings_dict['input_ids']\n",
    "        self.attn_masks = encodings_dict['attention_mask']\n",
    "#         self.labels = labels\n",
    "\n",
    "        # Calculate the position of the mask using self.input_ids\n",
    "        mask_id = tokenizer.encode(tokenizer.mask_token)[1] # 103\n",
    "        self.mask_pos = [sent_ids.index(mask_id) for sent_ids in self.input_ids]\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return_dict = {\"text_ids\": torch.tensor(self.input_ids[idx]),\n",
    "                       \"text_mask\": torch.tensor(self.attn_masks[idx]), \n",
    "                       \"mask_pos\": torch.tensor(self.mask_pos[idx]),\n",
    "#                        \"labels\": torch.tensor(self.labels[idx]).float()\n",
    "                      } \n",
    "        return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76a6455a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the test value result\n",
    "@torch.no_grad()\n",
    "\n",
    "def inference_fn_prompt(test_loader, model, device):\n",
    "    prediction=[]\n",
    "    true_prediction=[]\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    tk0 = tqdm(test_loader, total=len(test_loader))\n",
    "    for data in tk0:\n",
    "        text_ids = data['text_ids'].to(device, dtype = torch.long)\n",
    "        text_mask = data['text_mask'].to(device, dtype = torch.long)\n",
    "        mask_pos =data['mask_pos'].to(device, dtype = torch.long)\n",
    "#         targets = data['labels'].to(device, dtype=torch.long)\n",
    "        outputs = model(text_ids, text_mask,mask_pos)\n",
    "        prediction.append(F.softmax(outputs).to('cpu').numpy())\n",
    "#         true_prediction.append(targets.to('cpu').numpy())\n",
    "        # y_preds = y_preds.to('cpu').numpy()\n",
    "#         preds.append(y_preds)\n",
    "#     predictionss=np.argmax(np.array(prediction)[0],axis=1)\n",
    "#     print(print_statistics(np.array(true_prediction)[0],predictionss))\n",
    "#     print(f1_score(np.array(true_prediction)[0],predictionss, average='macro'))\n",
    "    prediction = np.concatenate(prediction)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "355c10b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTPromptV1(torch.nn.Module):\n",
    "    def __init__(self, bert, tokenizer, max_length=512, prompt_length=5):\n",
    "        super().__init__()\n",
    "        self.hidden_size = 768\n",
    "        self.max_length = max_length\n",
    "        self.prompt_length = prompt_length\n",
    "        \n",
    "        self.bert = bert.cuda()\n",
    "#         freeze(self.bert.embeddings)\n",
    "#         freeze(self.bert.encoder.layer[:2])\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.bert_embedding = self.bert.get_input_embeddings()\n",
    "        self.linear = torch.nn.Linear(self.hidden_size, 3).cuda()\n",
    "        self.act = torch.nn.Sigmoid() \n",
    "        \n",
    "        self.hidden_size = self.bert.config.hidden_size\n",
    "        self.LSTM = nn.LSTM(self.hidden_size,self.hidden_size,bidirectional=True)\n",
    "        self.clf = nn.Linear(self.hidden_size*2,3)\n",
    "        \n",
    "        # p tuning modules\n",
    "        self.prompt_embedding = torch.nn.Embedding(prompt_length, self.hidden_size).cuda()\n",
    "        self.lstm_head = torch.nn.LSTM(input_size=self.hidden_size, hidden_size=int(self.hidden_size/2),\n",
    "            num_layers=2, bidirectional=True, batch_first=True).cuda() # takes (batch_size, sequence length, hidden_size)\n",
    "        self.mlp_head = torch.nn.Sequential(torch.nn.Linear(self.hidden_size, self.hidden_size),\n",
    "                                            torch.nn.ReLU(),\n",
    "                                            torch.nn.Linear(self.hidden_size, self.hidden_size)).cuda()\n",
    "        \n",
    "    # calls the tokenize function to get input embeddings, then passes them through bert\n",
    "    def forward(self, input_sents):\n",
    "        embeds = torch.zeros(len(input_sents), self.max_length, self.hidden_size).cuda()\n",
    "        att_mask = torch.zeros(len(input_sents), self.max_length).cuda().long()\n",
    "        for i, sent in enumerate(input_sents):\n",
    "            embeds[i, :, :], att_mask[i, :] = self.tokenize(sent)\n",
    "        \n",
    "        output = self.bert(inputs_embeds=embeds, attention_mask=att_mask)[0]\n",
    "        cls_out = output[:, 0, :]\n",
    "        logits = self.linear(cls_out)\n",
    "        return logits\n",
    "    \n",
    "    # a tokenize function that embeds the sentence adds the prompt to the end and returns token embeddings along with attention mask\n",
    "    def tokenize(self, input_sent,prompt='   '):\n",
    "        # generate prompt tokens from embedding\n",
    "        self.prompt_length=len(tokenizer.encode(prompt))\n",
    "        prompt_tokens = self.prompt_embedding(torch.arange(self.prompt_length).cuda())\n",
    "        prompt_tokens = torch.unsqueeze(prompt_tokens, 0) # add a batch dimension\n",
    "        prompt_tokens, _ = self.lstm_head(prompt_tokens)\n",
    "        prompt_tokens = self.mlp_head(prompt_tokens)[0]\n",
    "        \n",
    "        # Encode the input_sentence\n",
    "        encoding_dict = self.tokenizer.encode_plus(input_sent, truncation=True, max_length=self.max_length, padding=\"max_length\")        \n",
    "#         input_ids = encoding_dict[\"input_ids\"]\n",
    "#         mask_id = tokenizer.encode(tokenizer.mask_token)[1] # 103\n",
    "        sep_pos = self.max_length - 1\n",
    "        \n",
    "        \n",
    "#         sep_pos = encoding_dict[\"input_ids\"].index(102) # the location of the [SEP] token is at the end of the input\n",
    "        token_embeds = self.bert_embedding(torch.tensor(encoding_dict[\"input_ids\"]).cuda())\n",
    "        \n",
    "        # Add the prompt tokens to the end and modify the att_mask to include the prompt\n",
    "        start_prompt_pos = min(sep_pos, self.max_length - self.prompt_length - 1) # if the sentence is truncutated we must further truncate it to fit in the prompt\n",
    "        end_prompt_pos = start_prompt_pos + self.prompt_length + 1\n",
    "        token_embeds[start_prompt_pos:end_prompt_pos, :] = torch.cat([prompt_tokens, token_embeds[sep_pos:sep_pos+1, :]], dim=0)\n",
    "        att_mask = encoding_dict[\"attention_mask\"]\n",
    "        att_mask[start_prompt_pos:end_prompt_pos] = [1]*(self.prompt_length+1)\n",
    "        att_mask = torch.tensor(att_mask).cuda()\n",
    "        \n",
    "        return token_embeds, att_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09d72e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dataset class for prompted BERT fine tuning\n",
    "# The dataset takes in query and passage, and then construct a training sample as:  <prompt> + [MASK] + <text>, returning the position of the mask\n",
    "# It also performs padding and stores the labels\n",
    "class BERTPromptDatasetV2(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset,  tokenizer, prompt=\"   \", max_length=512):\n",
    "        self.text=dataset['text']\n",
    "#         self.labels=labels\n",
    "        # Construct the input sentence\n",
    "#         input_sentences = [\"{} {} {}\".format(prompt, tokenizer.mask_token, text) for _, (text) in dataset.iterrows()]\n",
    "        \n",
    "#         # Encode and store\n",
    "#         encodings_dict = tokenizer.batch_encode_plus(input_sentences, truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "#         self.input_ids = encodings_dict['input_ids']\n",
    "#         self.attn_masks = encodings_dict['attention_mask']\n",
    "#         self.labels = labels\n",
    "\n",
    "#         # Calculate the position of the mask using self.input_ids\n",
    "#         mask_id = tokenizer.encode(tokenizer.mask_token)[1] # 103\n",
    "#         self.mask_pos = [sent_ids.index(mask_id) for sent_ids in self.input_ids]\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return_dict = {\"text\": self.text[idx],                       \n",
    "                       } \n",
    "        return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8dbec5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the test value result\n",
    "@torch.no_grad()\n",
    "\n",
    "def inference_fn_promptV2(test_loader, model, device):\n",
    "    prediction=[]\n",
    "    true_prediction=[]\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    tk0 = tqdm(test_loader, total=len(test_loader))\n",
    "    for data in tk0:\n",
    "        text_ids = data['text']\n",
    "#         text_mask = data['text_mask'].to(device, dtype = torch.long)\n",
    "#         mask_pos =data['mask_pos'].to(device, dtype = torch.long)\n",
    "#         targets = data['labels'].to(device, dtype=torch.long)\n",
    "        outputs = model(text_ids)\n",
    "        prediction.append(F.softmax(outputs).to('cpu').numpy())\n",
    "#         true_prediction.append(targets.to('cpu').numpy())\n",
    "        # y_preds = y_preds.to('cpu').numpy()\n",
    "#         preds.append(y_preds)\n",
    "#     predictionss=np.argmax(np.array(prediction)[0],axis=1)\n",
    "#     print(print_statistics(np.array(true_prediction)[0],predictionss))\n",
    "#     print(f1_score(np.array(true_prediction)[0],predictionss, average='macro'))\n",
    "    prediction = np.concatenate(prediction)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26624491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Plan:\n",
    "# take in the length of the prompt\n",
    "# create an embedding layer for that many tokens\n",
    "# create an LSTM and MLP to process the prompt \n",
    "# How to feed the embedding to BERT\n",
    "    # Get the embedding layer using https://huggingface.co/transformers/v3.0.2/model_doc/bert.html#transformers.BertModel.get_input_embeddings \n",
    "    # Then directly pass embedding to BERT using self.bert(input_embeds=, attention_mask=)\n",
    "# add prompting logic to the tokenizer function\n",
    "    # it needs to first pass all tokens through LSTM and MLP\n",
    "    # then add them into the sentence as needed\n",
    "class BERTPromptLSTM(torch.nn.Module):\n",
    "    def __init__(self, bert, tokenizer, max_length=512, prompt_length=5):\n",
    "        super().__init__()\n",
    "        self.hidden_size = 768\n",
    "        self.max_length = max_length\n",
    "        self.prompt_length = prompt_length\n",
    "        \n",
    "        self.bert = bert.cuda()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.bert_embedding = self.bert.get_input_embeddings()\n",
    "        self.linear = torch.nn.Linear(self.hidden_size, 3).cuda()\n",
    "        self.act = torch.nn.Sigmoid() \n",
    "        \n",
    "        self.hidden_size = self.bert.config.hidden_size\n",
    "        self.LSTM = nn.LSTM(self.hidden_size,self.hidden_size,bidirectional=True)\n",
    "        self.clf = nn.Linear(self.hidden_size*2,3)\n",
    "        \n",
    "        # p tuning modules\n",
    "        self.prompt_embedding = torch.nn.Embedding(prompt_length, self.hidden_size).cuda()\n",
    "        self.lstm_head = torch.nn.LSTM(input_size=self.hidden_size, hidden_size=int(self.hidden_size/2),\n",
    "            num_layers=2, bidirectional=True, batch_first=True).cuda() # takes (batch_size, sequence length, hidden_size)\n",
    "        self.mlp_head = torch.nn.Sequential(torch.nn.Linear(self.hidden_size, self.hidden_size),\n",
    "                                            torch.nn.ReLU(),\n",
    "                                            torch.nn.Linear(self.hidden_size, self.hidden_size)).cuda()\n",
    "        \n",
    "    # calls the tokenize function to get input embeddings, then passes them through bert\n",
    "    def forward(self, input_sents):\n",
    "        embeds = torch.zeros(len(input_sents), self.max_length, self.hidden_size).cuda()\n",
    "        att_mask = torch.zeros(len(input_sents), self.max_length).cuda().long()\n",
    "        for i, sent in enumerate(input_sents):\n",
    "            embeds[i, :, :], att_mask[i, :] = self.tokenize(sent)\n",
    "        \n",
    "        output = self.bert(inputs_embeds=embeds, attention_mask=att_mask)\n",
    "        encoded_layers = output[0].permute(1, 0, 2)\n",
    "        enc_hiddens, (last_hidden, last_cell) = self.LSTM(encoded_layers)\n",
    "        output_hidden = torch.cat((last_hidden[0], last_hidden[1]), dim=1)\n",
    "        output_hidden = F.dropout(output_hidden,0.2)\n",
    "        output = self.clf(output_hidden)\n",
    "        return output\n",
    "    \n",
    "    # a tokenize function that embeds the sentence adds the prompt to the end and returns token embeddings along with attention mask\n",
    "    def tokenize(self, input_sent,prompt='   '):\n",
    "        # generate prompt tokens from embedding\n",
    "        self.prompt_length=len(tokenizer.encode(prompt))\n",
    "        prompt_tokens = self.prompt_embedding(torch.arange(self.prompt_length).cuda())\n",
    "        prompt_tokens = torch.unsqueeze(prompt_tokens, 0) # add a batch dimension\n",
    "        prompt_tokens, _ = self.lstm_head(prompt_tokens)\n",
    "        prompt_tokens = self.mlp_head(prompt_tokens)[0]\n",
    "        \n",
    "        # Encode the input_sentence\n",
    "        encoding_dict = self.tokenizer.encode_plus(input_sent, truncation=True, max_length=self.max_length, padding=\"max_length\")        \n",
    "#         input_ids = encoding_dict[\"input_ids\"]\n",
    "#         mask_id = tokenizer.encode(tokenizer.mask_token)[1] # 103\n",
    "        sep_pos = self.max_length - 1\n",
    "        \n",
    "        \n",
    "#         sep_pos = encoding_dict[\"input_ids\"].index(102) # the location of the [SEP] token is at the end of the input\n",
    "        token_embeds = self.bert_embedding(torch.tensor(encoding_dict[\"input_ids\"]).cuda())\n",
    "        \n",
    "        # Add the prompt tokens to the end and modify the att_mask to include the prompt\n",
    "        start_prompt_pos = min(sep_pos, self.max_length - self.prompt_length - 1) # if the sentence is truncutated we must further truncate it to fit in the prompt\n",
    "        end_prompt_pos = start_prompt_pos + self.prompt_length + 1\n",
    "        token_embeds[start_prompt_pos:end_prompt_pos, :] = torch.cat([prompt_tokens, token_embeds[sep_pos:sep_pos+1, :]], dim=0)\n",
    "        att_mask = encoding_dict[\"attention_mask\"]\n",
    "        att_mask[start_prompt_pos:end_prompt_pos] = [1]*(self.prompt_length+1)\n",
    "        att_mask = torch.tensor(att_mask).cuda()\n",
    "        \n",
    "        return token_embeds, att_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de85b40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Plan:\n",
    "# take in the length of the prompt\n",
    "# create an embedding layer for that many tokens\n",
    "# create an LSTM and MLP to process the prompt \n",
    "# How to feed the embedding to BERT\n",
    "    # Get the embedding layer using https://huggingface.co/transformers/v3.0.2/model_doc/bert.html#transformers.BertModel.get_input_embeddings \n",
    "    # Then directly pass embedding to BERT using self.bert(input_embeds=, attention_mask=)\n",
    "# add prompting logic to the tokenizer function\n",
    "    # it needs to first pass all tokens through LSTM and MLP\n",
    "    # then add them into the sentence as needed\n",
    "class BERTPromptV3(torch.nn.Module):\n",
    "    def __init__(self, bert, tokenizer, max_length=512, prompt_length=5):\n",
    "        super().__init__()\n",
    "        self.hidden_size = 768\n",
    "        self.max_length = max_length\n",
    "        self.prompt_length = prompt_length\n",
    "        self.tokenizer=tokenizer\n",
    "        \n",
    "        self.bert = bert.cuda()\n",
    "        self.bert_embedding = self.bert.get_input_embeddings()\n",
    "        self.linear = torch.nn.Linear(self.hidden_size, 3).cuda()\n",
    "        self.act = torch.nn.Sigmoid() \n",
    "        \n",
    "        # p tuning modules\n",
    "        self.prompt_embedding = torch.nn.Embedding(prompt_length, self.hidden_size).cuda()\n",
    "        self.lstm_head = torch.nn.LSTM(input_size=self.hidden_size, hidden_size=int(self.hidden_size/2),\n",
    "            num_layers=2, bidirectional=True, batch_first=True).cuda() # takes (batch_size, sequence length, hidden_size)\n",
    "        self.mlp_head = torch.nn.Sequential(torch.nn.Linear(self.hidden_size, self.hidden_size),\n",
    "                                            torch.nn.ReLU(),\n",
    "                                            torch.nn.Linear(self.hidden_size, self.hidden_size)).cuda()\n",
    "        \n",
    "    # calls the tokenize function to get input embeddings, then passes them through bert\n",
    "    def forward(self, input_sents):\n",
    "        embeds = torch.zeros(len(input_sents), self.max_length, self.hidden_size).cuda()\n",
    "        att_mask = torch.zeros(len(input_sents), self.max_length).cuda().long()\n",
    "        for i, sent in enumerate(input_sents):\n",
    "            embeds[i, :, :], att_mask[i, :] = self.tokenize(sent)\n",
    "        \n",
    "        output = self.bert(inputs_embeds=embeds, attention_mask=att_mask)[0]\n",
    "        cls_out = output[:, 0, :]\n",
    "        logits = self.linear(cls_out)\n",
    "        return logits\n",
    "    \n",
    "    # a tokenize function that embeds the sentence adds the prompt to the end and returns token embeddings along with attention mask\n",
    "    def tokenize(self, input_sent,prompt='   '):\n",
    "        # generate prompt tokens from embedding\n",
    "        self.prompt_length=len(tokenizer.encode(prompt))\n",
    "        prompt_tokens = self.prompt_embedding(torch.arange(self.prompt_length).cuda())\n",
    "        prompt_tokens = torch.unsqueeze(prompt_tokens, 0) # add a batch dimension\n",
    "        prompt_tokens, _ = self.lstm_head(prompt_tokens)\n",
    "        prompt_tokens = self.mlp_head(prompt_tokens)[0]\n",
    "        \n",
    "        # Encode the input_sentence\n",
    "        encoding_dict = self.tokenizer.encode_plus(input_sent, truncation=True, max_length=self.max_length, padding=\"max_length\")        \n",
    "#         input_ids = encoding_dict[\"input_ids\"]\n",
    "#         mask_id = tokenizer.encode(tokenizer.mask_token)[1] # 103\n",
    "        sep_pos = self.max_length - 1\n",
    "        \n",
    "        \n",
    "#         sep_pos = encoding_dict[\"input_ids\"].index(102) # the location of the [SEP] token is at the end of the input\n",
    "        token_embeds = self.bert_embedding(torch.tensor(encoding_dict[\"input_ids\"]).cuda())\n",
    "        \n",
    "        # Add the prompt tokens to the end and modify the att_mask to include the prompt\n",
    "        start_prompt_pos = min(sep_pos, self.max_length - self.prompt_length - 1) # if the sentence is truncutated we must further truncate it to fit in the prompt\n",
    "        end_prompt_pos = start_prompt_pos + self.prompt_length + 1\n",
    "        token_embeds[start_prompt_pos:end_prompt_pos, :] = torch.cat([prompt_tokens, token_embeds[sep_pos:sep_pos+1, :]], dim=0)\n",
    "        att_mask = encoding_dict[\"attention_mask\"]\n",
    "        att_mask[start_prompt_pos:end_prompt_pos] = [1]*(self.prompt_length+1)\n",
    "        att_mask = torch.tensor(att_mask).cuda()\n",
    "        \n",
    "        return token_embeds, att_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf2cd614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "from transformers import BertModel,BertForMaskedLM\n",
    "class Bert_Model(nn.Module):\n",
    "    def __init__(self,  model_path ):\n",
    "        super(Bert_Model, self).__init__()\n",
    "        self.bert_config = AutoConfig.from_pretrained(model_path)\n",
    "\n",
    "        self.bert = AutoModelForMaskedLM.from_pretrained(model_path,config=self.bert_config )  \n",
    "#         freeze(self.bert.bert.embeddings)\n",
    "#         freeze(self.bert.bert.encoder.layer[:4])\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        outputs = self.bert(input_ids, attention_mask, token_type_ids)\n",
    "        logit = outputs[0]  \n",
    "\n",
    "\n",
    "        return logit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b7527662",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = '[MASK]  '\n",
    "MASK_POS = 0  \n",
    "\n",
    "def load_data1(tsvpath):\n",
    "    df=pd.read_csv(tsvpath, sep='\\t', lineterminator='\\n')\n",
    "    original_texts = df['#2_content'].tolist()\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    df['#3_label'] = le.fit_transform(df['#3_label'].values)\n",
    "    labels = df['#3_label'].tolist()\n",
    "\n",
    "    return original_texts,labels \n",
    "\n",
    "\n",
    "def load_data(tsvpath):\n",
    "    df=pd.read_csv(tsvpath)\n",
    "    original_texts = df['text'].tolist()\n",
    "#     from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#     le = LabelEncoder()\n",
    "#     df['#3_label'] = le.fit_transform(df['#3_label'].values)\n",
    "    labels = df['label'].tolist()\n",
    "\n",
    "    return original_texts,labels \n",
    "\n",
    "\n",
    "\n",
    "def ProcessData(filepath,tokenizer,train=True):\n",
    "    pos_id=tokenizer.convert_tokens_to_ids(\"\") #9005\n",
    "    neg_id=tokenizer.convert_tokens_to_ids(\"\")  #12139\n",
    "    neu_id=tokenizer.convert_tokens_to_ids(\"\")  #12139\n",
    "    if train:\n",
    "        x_train,y_train=load_data(filepath)\n",
    "    else:\n",
    "        x_train,y_train=load_data1(filepath)\n",
    "    #x_train,x_test,y_train,y_test=train_test_split(StrongData,StrongLabel,test_size=0.3, random_state=42)\n",
    "\n",
    "    Inputid=[]\n",
    "    Labelid=[]\n",
    "    typeid=[]\n",
    "    attenmask=[]\n",
    "\n",
    "    for i in range(len(x_train)):\n",
    "\n",
    "        text_ = PREFIX+x_train[i]\n",
    "\n",
    "        encode_dict = tokenizer.encode_plus(text_,max_length=512,padding=\"max_length\",truncation=True)\n",
    "        input_ids=encode_dict[\"input_ids\"]\n",
    "        type_ids=encode_dict[\"token_type_ids\"]\n",
    "        atten_mask=encode_dict[\"attention_mask\"]\n",
    "        labelid,inputid= input_ids[:],input_ids[:]\n",
    "        if y_train[i] == 0:\n",
    "            # print('jjjj')\n",
    "            labelid[MASK_POS] = neg_id\n",
    "            # print(tokenizer.convert_ids_to_tokens(neg_id))\n",
    "            labelid[:MASK_POS] = [-1]* len(labelid[:MASK_POS]) \n",
    "            labelid[MASK_POS+1:] = [-1] * len(labelid[MASK_POS+1:])\n",
    "            inputid[MASK_POS] = tokenizer.mask_token_id\n",
    "        elif y_train[i] == 1:\n",
    "            labelid[MASK_POS] = neu_id\n",
    "            labelid[:MASK_POS] = [-1]* len(labelid[:MASK_POS]) \n",
    "            labelid[MASK_POS+1:] = [-1] * len(labelid[MASK_POS+1:])\n",
    "            inputid[MASK_POS] = tokenizer.mask_token_id\n",
    "        else:\n",
    "            labelid[MASK_POS] = pos_id\n",
    "            labelid[:MASK_POS] = [-1]* len(labelid[:MASK_POS]) \n",
    "            labelid[MASK_POS+1:] = [-1] * len(labelid[MASK_POS+1:])\n",
    "            inputid[MASK_POS] = tokenizer.mask_token_id\n",
    "\n",
    "        Labelid.append(labelid)\n",
    "        Inputid.append(inputid)\n",
    "        typeid.append(type_ids)\n",
    "        attenmask.append(atten_mask)\n",
    "\n",
    "    return Inputid,Labelid,typeid,attenmask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e570ad9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = '[MASK]  '\n",
    "MASK_POS = 0  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_data_test(tsvpath):\n",
    "    df=pd.read_csv(tsvpath, sep='\\t', lineterminator='\\n')\n",
    "    original_texts = df['#2_content'].tolist()\n",
    "\n",
    "    return original_texts \n",
    "\n",
    "\n",
    "\n",
    "def ProcessDataTest(filepath,tokenizer):\n",
    "    pos_id=tokenizer.convert_tokens_to_ids(\"\") #9005\n",
    "    neg_id=tokenizer.convert_tokens_to_ids(\"\")  #12139\n",
    "    neu_id=tokenizer.convert_tokens_to_ids(\"\")  #12139\n",
    "    x_train=load_data_test(filepath)\n",
    "   \n",
    "    #x_train,x_test,y_train,y_test=train_test_split(StrongData,StrongLabel,test_size=0.3, random_state=42)\n",
    "\n",
    "    Inputid=[]\n",
    "    typeid=[]\n",
    "    attenmask=[]\n",
    "\n",
    "    for i in range(len(x_train)):\n",
    "\n",
    "        text_ = PREFIX+x_train[i]\n",
    "\n",
    "        encode_dict = tokenizer.encode_plus(text_,max_length=512,padding=\"max_length\",truncation=True)\n",
    "        input_ids=encode_dict[\"input_ids\"]\n",
    "        type_ids=encode_dict[\"token_type_ids\"]\n",
    "        atten_mask=encode_dict[\"attention_mask\"]\n",
    "        labelid,inputid= input_ids[:],input_ids[:]\n",
    "        inputid[MASK_POS] = tokenizer.mask_token_id\n",
    "#         if y_train[i] == 0:\n",
    "#             # print('jjjj')\n",
    "#             labelid[MASK_POS] = neg_id\n",
    "#             # print(tokenizer.convert_ids_to_tokens(neg_id))\n",
    "#             labelid[:MASK_POS] = [-1]* len(labelid[:MASK_POS]) \n",
    "#             labelid[MASK_POS+1:] = [-1] * len(labelid[MASK_POS+1:])\n",
    "#             inputid[MASK_POS] = tokenizer.mask_token_id\n",
    "#         elif y_train[i] == 1:\n",
    "# #             labelid[MASK_POS] = neu_id\n",
    "# #             labelid[:MASK_POS] = [-1]* len(labelid[:MASK_POS]) \n",
    "# #             labelid[MASK_POS+1:] = [-1] * len(labelid[MASK_POS+1:])\n",
    "#             inputid[MASK_POS] = tokenizer.mask_token_id\n",
    "#         else:\n",
    "# #             labelid[MASK_POS] = pos_id\n",
    "# #             labelid[:MASK_POS] = [-1]* len(labelid[:MASK_POS]) \n",
    "# #             labelid[MASK_POS+1:] = [-1] * len(labelid[MASK_POS+1:])\n",
    "#             inputid[MASK_POS] = tokenizer.mask_token_id\n",
    "\n",
    "#         Labelid.append(labelid)\n",
    "        Inputid.append(inputid)\n",
    "        typeid.append(type_ids)\n",
    "        attenmask.append(atten_mask)\n",
    "\n",
    "    return Inputid,typeid,attenmask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "585e5685",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as Data\n",
    "\n",
    "class MyDataSet(Data.Dataset):\n",
    "    def __init__(self, sen , mask , typ ,label ):\n",
    "        super(MyDataSet, self).__init__()\n",
    "        self.sen = torch.tensor(sen,dtype=torch.long)\n",
    "        self.mask = torch.tensor(mask,dtype=torch.long)\n",
    "        self.typ =torch.tensor( typ,dtype=torch.long)\n",
    "        self.label = torch.tensor(label,dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.sen.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sen[idx], self.mask[idx],self.typ[idx],self.label[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "edd7eaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as Data\n",
    "\n",
    "class MyDataSetTest(Data.Dataset):\n",
    "    def __init__(self, sen , mask , typ  ):\n",
    "        super(MyDataSetTest, self).__init__()\n",
    "        self.sen = torch.tensor(sen,dtype=torch.long)\n",
    "        self.mask = torch.tensor(mask,dtype=torch.long)\n",
    "        self.typ =torch.tensor( typ,dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.sen.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sen[idx], self.mask[idx],self.typ[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4c7eb8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the test value result\n",
    "@torch.no_grad()\n",
    "\n",
    "def inference_fn_promptV3(test_loader, model, device):\n",
    "    prediction=[]\n",
    "    MASK_POS=0\n",
    "    true_prediction=[]\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "#     tk0 = tqdm(test_loader, total=len(test_loader))\n",
    "    for idx,(ids,att_mask,type) in enumerate(test_loader):\n",
    "        ids,att_mask,type= ids.to(device),att_mask.to(device),type.to(device)\n",
    "        out_train = model(ids,att_mask,type)\n",
    "        tout_train_mask = out_train[:, MASK_POS, :]\n",
    "            # print(tout_train_mask.data)\n",
    "        predicted_test = torch.max(tout_train_mask, 1)[1]\n",
    "        prediction.append(predicted_test.to('cpu').numpy())\n",
    "\n",
    "    prediction = np.concatenate(prediction)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aee3655",
   "metadata": {},
   "source": [
    "# Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "67019bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1 = pd.read_csv('/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/Dataset/NADI2022_Subtask1_TRAIN.tsv', sep='\\t', lineterminator='\\n')\n",
    "train_2 = pd.read_csv('/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/Dataset/NADI2022_Subtask2_TRAIN.tsv', sep='\\t', lineterminator='\\n')\n",
    "valid_1 = pd.read_csv('/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/Dataset/NADI2022_Subtask1_DEV.tsv', sep='\\t', lineterminator='\\n')\n",
    "valid_2 = pd.read_csv('/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/Dataset/NADI2022_Subtask2_DEV.tsv', sep='\\t', lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "31f90125",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1.rename(columns = {'#2_content':'text', '#3_label':'label'}, inplace = True)\n",
    "train_1['task']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6d5352c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2.rename(columns = {'#2_content':'text', '#3_label':'label'}, inplace = True)\n",
    "train_2['task']=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a5745a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_1.rename(columns = {'#2_content':'text', '#3_label':'label'}, inplace = True)\n",
    "valid_1['task']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c24141",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9b72e252",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_2.rename(columns = {'#2_content':'text', '#3_label':'label'}, inplace = True)\n",
    "valid_2['task']=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b3f5ac30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "train_1['label'] = le.fit_transform(train_1['label'].values)\n",
    "valid_1['label'] = le.transform(valid_1['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2005e687",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "train_2['label'] = le.fit_transform(train_2['label'].values)\n",
    "le = LabelEncoder()\n",
    "\n",
    "valid_2['label'] = le.fit_transform(valid_2['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dff8d121",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbalizer = {i: [l] for (i, l) in enumerate(le.classes_)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6d1c6a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ['neg'], 1: ['neut'], 2: ['pos']}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verbalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6d08792a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e738da6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Prediction_final=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "abb15278",
   "metadata": {},
   "outputs": [],
   "source": [
    "test= pd.read_csv('/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/Dataset/NADI2022_Subtask2_TEST_Unlabeled.tsv', sep='\\t', lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "34750575",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.rename(columns = {'#2_content':'text'}, inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ce47dc",
   "metadata": {},
   "source": [
    "## AraElectra MTL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598b7a23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e0715341",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pth=['/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/MTL/Loss_F1-Fold-0.bin','/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/MTL/Loss_F1-Fold-1.bin','/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/MTL/Loss_F1-Fold-2.bin','/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/MTL/Loss_F1-Fold-3.bin','/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/MTL/Loss_F1-Fold-4.bin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e5390b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at aubmindlab/araelectra-base-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer_electra= AutoTokenizer.from_pretrained('aubmindlab/araelectra-base-discriminator')\n",
    "model_electra=NADIModelMTL('aubmindlab/araelectra-base-discriminator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c5df4996",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:28<00:00, 10.72it/s]\n",
      "  0%|                                                                                           | 0/305 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|| 305/305 [00:28<00:00, 10.76it/s]\n",
      "  0%|                                                                                           | 0/305 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|| 305/305 [00:28<00:00, 10.69it/s]\n",
      "  0%|                                                                                           | 0/305 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|| 305/305 [00:28<00:00, 10.66it/s]\n",
      "  0%|                                                                                           | 0/305 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|| 305/305 [00:28<00:00, 11.57it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|| 305/305 [00:28<00:00, 10.64it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "testDataset = TestDataset(valid_1, tokenizer_electra)\n",
    "test_loader = DataLoader(testDataset,\n",
    "                              batch_size =16,\n",
    "                              shuffle=False,\n",
    "                              num_workers = 4,\n",
    "                              pin_memory = True,\n",
    "                              drop_last=False)\n",
    "\n",
    "electra_MTL_predictions=[]\n",
    "for pth in model_pth:\n",
    "    model_electra.load_state_dict(torch.load(pth))\n",
    "    prediction = inference_fn_MTL_task_1(test_loader, model_electra, device)\n",
    "    electra_MTL_predictions.append(prediction)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "electra_MTL_predictions=np.argmax(np.mean(np.array(electra_MTL_predictions),axis=0),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7d5690c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(print_statistics(valid_1['label'],electra_MTL_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "15943b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/188 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|| 188/188 [00:17<00:00, 10.61it/s]\n",
      "  0%|                                                                                           | 0/188 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|| 188/188 [00:17<00:00, 10.60it/s]\n",
      "  0%|                                                                                           | 0/188 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|| 188/188 [00:17<00:00, 10.60it/s]\n",
      "  0%|                                                                                           | 0/188 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|| 188/188 [00:17<00:00, 10.60it/s]\n",
      "  0%|                                                                                           | 0/188 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|| 188/188 [00:17<00:00, 10.59it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "testDataset = TestDataset(test, tokenizer_electra)\n",
    "test_loader = DataLoader(testDataset,\n",
    "                              batch_size =16,\n",
    "                              shuffle=False,\n",
    "                              num_workers = 4,\n",
    "                              pin_memory = True,\n",
    "                              drop_last=False)\n",
    "\n",
    "electra_MTL_predictions=[]\n",
    "for pth in model_pth:\n",
    "    model_electra.load_state_dict(torch.load(pth))\n",
    "    prediction = inference_fn_MTL_task_2(test_loader, model_electra, device)\n",
    "    electra_MTL_predictions.append(prediction)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "Prediction_final.append(np.mean(np.array(electra_MTL_predictions),axis=0))\n",
    "electra_MTL_predictions=np.argmax(np.mean(np.array(electra_MTL_predictions),axis=0),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "805d3397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(print_statistics(valid_2['label'],electra_MTL_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "432f1252",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_electra, tokenizer_electra\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9488adf3",
   "metadata": {},
   "source": [
    "## Marbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7852ddf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NADIModelMarbert(nn.Module):\n",
    "  def __init__(self, pretrained_path='aubmindlab/bert-base-arabert',in_feature=768, class_num=3, dropout_prob=0.2):\n",
    "        super(NADIModelMarbert, self).__init__()\n",
    "        self.base_model=TransformerLayer(pretrained_path).to('cuda')\n",
    "        self.config = AutoConfig.from_pretrained(pretrained_path)\n",
    "        self.weighted_pooler = WeightedLayerPooling(num_hidden_layers=self.config.num_hidden_layers, layer_start=4)\n",
    "        self.att=Attention(768, 256).to('cuda')\n",
    "        self.classifier=nn.Linear(768, class_num)\n",
    "  def forward(self, ids,mask):\n",
    "    output=self.base_model(ids,mask)\n",
    "    all_hidden_states = torch.stack(output.hidden_states)\n",
    "    weighted_pooling_embeddings = (self.weighted_pooler(all_hidden_states)) # For WeightedLayerPooling\n",
    "    # weighted_pooling_embeddings = weighted_pooling_embeddings[:, 0]\n",
    "    # print(weighted_pooling_embeddings.shape)\n",
    "    output=self.att(weighted_pooling_embeddings)\n",
    "    output=self.classifier(output)\n",
    "    # output=torch.softmax(output, dim=1)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ba8eb91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDatasetMrbert(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=512):\n",
    "        self.df = df\n",
    "        self.max_len = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text = df['text'].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.text[index]\n",
    "        # summary = self.summary[index]\n",
    "        inputs_text = self.tokenizer.encode_plus(\n",
    "                                text,\n",
    "                                truncation=True,\n",
    "                                add_special_tokens=True,\n",
    "                                max_length=self.max_len,\n",
    "                                padding='max_length'\n",
    "                            )\n",
    "        \n",
    "                            \n",
    "        \n",
    "        text_ids = inputs_text['input_ids']\n",
    "        text_mask = inputs_text['attention_mask']\n",
    "        token_type_ids=inputs_text['token_type_ids']\n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "        return {\n",
    "            \n",
    "            'input_ids': torch.tensor(text_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(text_mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "762f2e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at UBC-NLP/MARBERTv2 were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_pth=['/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/Marbert/Loss_F1-Fold-0.bin','/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/Marbert/Loss_F1-Fold-1.bin','/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/Marbert/Loss_F1-Fold-2.bin','/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/Marbert/Loss_F1-Fold-3.bin','/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/Marbert/Loss_F1-Fold-4.bin']\n",
    "tokenizer_marbert= AutoTokenizer.from_pretrained('UBC-NLP/MARBERTv2')\n",
    "\n",
    "model_marbert =  NADIModelMarbert('UBC-NLP/MARBERTv2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ddb60808",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/188 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|| 188/188 [00:08<00:00, 22.96it/s]\n",
      "  0%|                                                                                           | 0/188 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|| 188/188 [00:08<00:00, 22.92it/s]\n",
      "  0%|                                                                                           | 0/188 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|| 188/188 [00:08<00:00, 22.89it/s]\n",
      "  0%|                                                                                           | 0/188 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|| 188/188 [00:08<00:00, 22.90it/s]\n",
      "  0%|                                                                                           | 0/188 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|| 188/188 [00:08<00:00, 22.93it/s]\n"
     ]
    }
   ],
   "source": [
    "device=\"cuda:0\"\n",
    "testDataset = TestDatasetMrbert(test, tokenizer_marbert,max_length=256)\n",
    "test_loader = DataLoader(testDataset,\n",
    "                              batch_size =16,\n",
    "                              shuffle=False,\n",
    "                              num_workers = 4,\n",
    "                              pin_memory = True,\n",
    "                              drop_last=False)\n",
    "\n",
    "marbert_predictions=[]\n",
    "for pth in model_pth:\n",
    "    model_marbert.load_state_dict(torch.load(pth))\n",
    "    prediction = inference_fn(test_loader, model_marbert, device)\n",
    "    marbert_predictions.append(prediction)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "Prediction_final.append(np.mean(np.array(marbert_predictions),axis=0))\n",
    "\n",
    "marbert_predictions=np.argmax(np.mean(np.array(marbert_predictions),axis=0),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d3efbcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(print_statistics(valid_2['label'],marbert_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "66e2c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_marbert,tokenizer_marbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b90155f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "530a2af4",
   "metadata": {},
   "source": [
    "# Marbert With features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d149fbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at UBC-NLP/MARBERTv2 were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "valid_features=  pd.read_csv('/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/Dataset/test_subtask2_with_features.csv')\n",
    "tokenizer_marbert= AutoTokenizer.from_pretrained('UBC-NLP/MARBERTv2')\n",
    "model_pth=['/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/Marbert/Loss_lstm__-0.bin']\n",
    "model_marbert =  NADIModelLSTMFeatures('UBC-NLP/MARBERTv2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "979c2704",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/188 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|| 188/188 [00:17<00:00, 10.86it/s]\n"
     ]
    }
   ],
   "source": [
    "device=\"cuda:0\"\n",
    "testDataset = TrainDatasetFeatures(valid_features, tokenizer_marbert,max_length=256)\n",
    "test_loader = DataLoader(testDataset,\n",
    "                              batch_size =16,\n",
    "                              shuffle=False,\n",
    "                              num_workers = 4,\n",
    "                              pin_memory = True,\n",
    "                              drop_last=False)\n",
    "\n",
    "marbert_predictions=[]\n",
    "for pth in model_pth:\n",
    "    model_marbert.load_state_dict(torch.load(pth))\n",
    "    prediction = inference_fn_marbert_features(test_loader, model_marbert, device)\n",
    "    marbert_predictions.append(prediction)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "predictions_5_tokeepp=np.array(marbert_predictions)[0]\n",
    "Prediction_final.append(np.mean(np.array(marbert_predictions),axis=0))\n",
    "\n",
    "marbert_predictions=np.argmax(np.mean(np.array(marbert_predictions),axis=0),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "575722c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(print_statistics(valid_2['label'],marbert_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cdb40b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_marbert,tokenizer_marbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4786f1a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5b60adc",
   "metadata": {},
   "source": [
    "#  Arabert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f9d3f462",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at aubmindlab/bert-base-arabertv02-twitter were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02-twitter and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_pth=['/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/Arabert/Loss_F1-Fold-0.bin','/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/Arabert/Loss_F1-Fold-1.bin','/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/Arabert/Loss_F1-Fold-2.bin','/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/Arabert/Loss_F1-Fold-3.bin','/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/Arabert/Loss_F1-Fold-4.bin']\n",
    "tokenizer_arabert= AutoTokenizer.from_pretrained('aubmindlab/bert-base-arabertv02-twitter')\n",
    "\n",
    "model_arabert = NADIModelArabert('aubmindlab/bert-base-arabertv02-twitter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ad986029",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/188 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|| 188/188 [00:17<00:00, 10.63it/s]\n",
      "  0%|                                                                                           | 0/188 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|| 188/188 [00:17<00:00, 10.60it/s]\n",
      "  0%|                                                                                           | 0/188 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|| 188/188 [00:17<00:00, 10.58it/s]\n",
      "  0%|                                                                                           | 0/188 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|| 188/188 [00:17<00:00, 10.59it/s]\n",
      "  0%|                                                                                           | 0/188 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|| 188/188 [00:17<00:00, 10.57it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "testDataset = TestDataset(test, tokenizer_arabert)\n",
    "test_loader = DataLoader(testDataset,\n",
    "                              batch_size =16,\n",
    "                              shuffle=False,\n",
    "                              num_workers = 4,\n",
    "                              pin_memory = True,\n",
    "                              drop_last=False)\n",
    "\n",
    "arabert_predictions=[]\n",
    "for pth in model_pth:\n",
    "    model_arabert.load_state_dict(torch.load(pth))\n",
    "    prediction = inference_fn(test_loader, model_arabert, device)\n",
    "    arabert_predictions.append(prediction)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "Prediction_final.append(np.mean(np.array(arabert_predictions),axis=0))\n",
    "\n",
    "arabert_predictions=np.argmax(np.mean(np.array(arabert_predictions),axis=0),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8ef113",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "71d48e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(print_statistics(valid_2['label'],arabert_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9439b460",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_arabert,tokenizer_arabert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49c2737",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e5844e9",
   "metadata": {},
   "source": [
    "## AraElectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "53bf15ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at aubmindlab/araelectra-base-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_pth=['/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/araelectra/Loss_F1-Fold-0.bin','/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/araelectra/Loss_F1-Fold-1.bin','/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/araelectra/Loss_F1-Fold-2.bin','/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/araelectra/Loss_F1-Fold-3.bin','/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/araelectra/Loss_F1-Fold-4.bin']\n",
    "tokenizer_electra= AutoTokenizer.from_pretrained('aubmindlab/araelectra-base-discriminator')\n",
    "\n",
    "model_electra = NADIModelElectra('aubmindlab/araelectra-base-discriminator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dd0e8cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/188 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|| 188/188 [00:17<00:00, 10.81it/s]\n",
      "  0%|                                                                                           | 0/188 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|| 188/188 [00:17<00:00, 10.79it/s]\n",
      "  0%|                                                                                           | 0/188 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|| 188/188 [00:17<00:00, 10.77it/s]\n",
      "  0%|                                                                                           | 0/188 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|| 188/188 [00:17<00:00, 10.77it/s]\n",
      "  0%|                                                                                           | 0/188 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|| 188/188 [00:17<00:00, 10.75it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "testDataset = TestDataset(test, tokenizer_electra)\n",
    "test_loader = DataLoader(testDataset,\n",
    "                              batch_size =16,\n",
    "                              shuffle=False,\n",
    "                              num_workers = 4,\n",
    "                              pin_memory = True,\n",
    "                              drop_last=False)\n",
    "\n",
    "electra_predictions=[]\n",
    "for pth in model_pth:\n",
    "    model_electra.load_state_dict(torch.load(pth))\n",
    "    prediction = inference_fn(test_loader, model_electra, device)\n",
    "    electra_predictions.append(prediction)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "Prediction_final.append(np.mean(np.array(electra_predictions),axis=0))\n",
    "\n",
    "electra_predictions=np.argmax(np.mean(np.array(electra_predictions),axis=0),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e7be47cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(print_statistics(valid_2['label'],electra_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da63164e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6a8e991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_electra,tokenizer_electra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2050be",
   "metadata": {},
   "source": [
    "## One fold AraElectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "24a1231e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at aubmindlab/araelectra-base-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer_electra= AutoTokenizer.from_pretrained('aubmindlab/araelectra-base-discriminator')\n",
    "model_pth=['/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/araelectra/Loss.bin']\n",
    "\n",
    "model_electra = NADIModelElectra('aubmindlab/araelectra-base-discriminator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2695edc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/188 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|| 188/188 [00:17<00:00, 10.80it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "testDataset = TestDataset(test, tokenizer_electra)\n",
    "test_loader = DataLoader(testDataset,\n",
    "                              batch_size =16,\n",
    "                              shuffle=False,\n",
    "                              num_workers = 4,\n",
    "                              pin_memory = True,\n",
    "                              drop_last=False)\n",
    "\n",
    "electra_predictions=[]\n",
    "for pth in model_pth:\n",
    "    model_electra.load_state_dict(torch.load(pth))\n",
    "    prediction = inference_fn(test_loader, model_electra, device)\n",
    "    electra_predictions.append(prediction)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "Prediction_final.append(np.mean(np.array(electra_predictions),axis=0))\n",
    "\n",
    "electra_predictions=np.argmax(np.mean(np.array(electra_predictions),axis=0),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a12659a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(print_statistics(valid_2['label'],electra_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6dafbca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_electra,tokenizer_electra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843e5854",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4488f6be",
   "metadata": {},
   "source": [
    "# Prompt Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "99d7d364",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/Dataset/NADI2022_Subtask2_TRAIN.tsv', sep='\\t', lineterminator='\\n')\n",
    "valid = pd.read_csv('/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/Dataset/NADI2022_Subtask2_DEV.tsv', sep='\\t', lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8996f6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "train['#3_label'] = le.fit_transform(train['#3_label'].values)\n",
    "valid['#3_label'] = le.transform(valid['#3_label'].values)\n",
    "train.rename(columns = {'#2_content':'text', '#3_label':'label'}, inplace = True)\n",
    "valid.rename(columns = {'#2_content':'text', '#3_label':'label'}, inplace = True)\n",
    "test.rename(columns = {'#2_content':'text'}, inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66658295",
   "metadata": {},
   "source": [
    "### Arabert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0f73efbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at aubmindlab/bert-base-arabertv02-twitter were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02-twitter and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer= AutoTokenizer.from_pretrained('aubmindlab/bert-base-arabertv02-twitter')\n",
    "arabert_model=AutoModel.from_pretrained('aubmindlab/bert-base-arabertv02-twitter')\n",
    "model_pth=[\"/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/Arabert_prompt/Loss.bin\"]\n",
    "arabert_prompt = BERTPrompt(arabert_model, tokenizer)\n",
    "testDataset = BERTPromptDataset(test[['text']], tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6adfdd42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arabert_prompt.load_state_dict(torch.load(model_pth[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cbaac171",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/750 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                  | 3/750 [00:00<00:31, 23.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 747/750 [00:19<00:00, 37.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 750/750 [00:19<00:00, 37.70it/s]\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(testDataset, batch_size=4, \n",
    "                              num_workers=2, shuffle=False, pin_memory=True)\n",
    "arabert_predictions=[]\n",
    "# for pth in model_pth:\n",
    "# arabert_prompt.load_state_dict(torch.load(pth))\n",
    "prediction = inference_fn_prompt(test_loader, arabert_prompt, device)\n",
    "arabert_predictions.append(prediction)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "Prediction_final.append(np.array(arabert_predictions)[0])\n",
    "\n",
    "arabert_predictions=np.argmax(np.array(arabert_predictions)[0],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ba038328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(print_statistics(valid_2['label'],arabert_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dbfe7e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "del tokenizer,arabert_prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6480c43",
   "metadata": {},
   "source": [
    "### AraElectra Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fbdaefff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at aubmindlab/araelectra-base-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer= AutoTokenizer.from_pretrained('aubmindlab/araelectra-base-discriminator')\n",
    "araelectra_model=AutoModel.from_pretrained('aubmindlab/araelectra-base-discriminator')\n",
    "model_pth=[\"/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/araelectra_prompt/Loss_new.bin\"]\n",
    "araelectra_prompt = BERTPrompt(araelectra_model, tokenizer)\n",
    "testDataset = BERTPromptDataset(test[['text']],  tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7472575d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "araelectra_prompt.load_state_dict(torch.load(model_pth[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "07bbcdd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/750 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  1%|                                                                                  | 6/750 [00:00<00:24, 30.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 750/750 [00:19<00:00, 37.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 750/750 [00:19<00:00, 37.75it/s]\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(testDataset, batch_size=4, \n",
    "                              num_workers=2, shuffle=False, pin_memory=True)\n",
    "araelectra_predictions=[]\n",
    "# for pth in model_pth:\n",
    "# arabert_prompt.load_state_dict(torch.load(pth))\n",
    "prediction = inference_fn_prompt(test_loader, araelectra_prompt, device)\n",
    "araelectra_predictions.append(prediction)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "Prediction_final.append(np.array(araelectra_predictions)[0])\n",
    "\n",
    "araelectra_predictions=np.argmax(np.array(araelectra_predictions)[0],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "81c8c441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(print_statistics(valid_2['label'],araelectra_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9f5a6307",
   "metadata": {},
   "outputs": [],
   "source": [
    "del araelectra_prompt,tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee0ed71",
   "metadata": {},
   "source": [
    "## Prompt Tunning V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17635282",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b9acb3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at UBC-NLP/MARBERTv2 were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer= AutoTokenizer.from_pretrained('UBC-NLP/MARBERTv2')\n",
    "arabert_model=AutoModel.from_pretrained('UBC-NLP/MARBERTv2')\n",
    "pBERT = BERTPromptLSTM(arabert_model, tokenizer, prompt_length=25)\n",
    "testDataset = BERTPromptDatasetV2(test[['text']], tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ab90d430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pth=[\"/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/marbert_prompt/Loss_lstm.bin\"]\n",
    "pBERT.load_state_dict(torch.load(model_pth[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fa4411e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/750 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                  | 3/750 [00:00<00:47, 15.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 749/750 [00:38<00:00, 19.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 750/750 [00:38<00:00, 19.62it/s]\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(testDataset, batch_size=4, \n",
    "                              num_workers=2, shuffle=False, pin_memory=True)\n",
    "marbert_prompt_predictions=[]\n",
    "# for pth in model_pth:\n",
    "# arabert_prompt.load_state_dict(torch.load(pth))\n",
    "prediction = inference_fn_promptV2(test_loader, pBERT, device)\n",
    "marbert_prompt_predictions.append(prediction)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# Prediction_final.append(np.mean((np.array(marbert_prompt_predictions)[0]),axis=0))\n",
    "\n",
    "predictions_4_tokeepp=np.array(marbert_prompt_predictions)[0]\n",
    "Prediction_final.append(predictions_4_tokeepp)\n",
    "\n",
    "\n",
    "marbert_prompt_predictions=np.argmax(np.array(marbert_prompt_predictions)[0],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d6a1a726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(print_statistics(valid_2['label'],marbert_prompt_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc646c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a139a642",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at UBC-NLP/MARBERTv2 were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer= AutoTokenizer.from_pretrained('UBC-NLP/MARBERTv2')\n",
    "arabert_model=AutoModel.from_pretrained('UBC-NLP/MARBERTv2')\n",
    "pBERT = BERTPromptV3(arabert_model, tokenizer, prompt_length=25)\n",
    "testDataset = BERTPromptDatasetV2(test[['text']], tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0855cc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/750 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                  | 2/750 [00:00<00:45, 16.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 750/750 [00:23<00:00, 31.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 750/750 [00:23<00:00, 31.67it/s]\n"
     ]
    }
   ],
   "source": [
    "model_pth=[\"/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/marbert_prompt/Loss_new.bin\"]\n",
    "pBERT.load_state_dict(torch.load(model_pth[0]))\n",
    "test_loader = DataLoader(testDataset, batch_size=4, \n",
    "                              num_workers=2, shuffle=False, pin_memory=True)\n",
    "marbert_prompt_predictions=[]\n",
    "# for pth in model_pth:\n",
    "# arabert_prompt.load_state_dict(torch.load(pth))\n",
    "prediction = inference_fn_promptV2(test_loader, pBERT, device)\n",
    "marbert_prompt_predictions.append(prediction)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "predictions_3_tokeepp=np.array(marbert_prompt_predictions)[0]\n",
    "Prediction_final.append(predictions_3_tokeepp)\n",
    "\n",
    "\n",
    "marbert_prompt_predictions=np.argmax(np.array(marbert_prompt_predictions)[0],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "866bec8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(print_statistics(valid_2['label'],marbert_prompt_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca3e758",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ad002a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/750 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                  | 2/750 [00:00<00:45, 16.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 750/750 [00:23<00:00, 31.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 750/750 [00:23<00:00, 31.71it/s]\n"
     ]
    }
   ],
   "source": [
    "model_pth=[\"/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/marbert_prompt/Loss.bin\"]\n",
    "pBERT.load_state_dict(torch.load(model_pth[0]))\n",
    "test_loader = DataLoader(testDataset, batch_size=4, \n",
    "                              num_workers=2, shuffle=False, pin_memory=True)\n",
    "marbert_prompt_predictions=[]\n",
    "# for pth in model_pth:\n",
    "# arabert_prompt.load_state_dict(torch.load(pth))\n",
    "prediction = inference_fn_promptV2(test_loader, pBERT, device)\n",
    "marbert_prompt_predictions.append(prediction)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "marbert_prompt_predictions=np.argmax(np.array(marbert_prompt_predictions)[0],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "44ed3524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(print_statistics(valid_2['label'],marbert_prompt_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56826c05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2d5975df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/750 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                  | 2/750 [00:00<00:45, 16.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 750/750 [00:23<00:00, 31.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 750/750 [00:23<00:00, 31.62it/s]\n"
     ]
    }
   ],
   "source": [
    "model_pth=[\"/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/marbert_prompt/Loss_new_modifoed.bin\"]\n",
    "pBERT.load_state_dict(torch.load(model_pth[0]))\n",
    "test_loader = DataLoader(testDataset, batch_size=4, \n",
    "                              num_workers=2, shuffle=False, pin_memory=True)\n",
    "marbert_prompt_predictions=[]\n",
    "# for pth in model_pth:\n",
    "# arabert_prompt.load_state_dict(torch.load(pth))\n",
    "prediction = inference_fn_promptV2(test_loader, pBERT, device)\n",
    "marbert_prompt_predictions.append(prediction)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "predictions_2_tokeepp=np.array(marbert_prompt_predictions)[0]\n",
    "Prediction_final.append(predictions_2_tokeepp)\n",
    "\n",
    "marbert_prompt_predictions=np.argmax(np.array(marbert_prompt_predictions)[0],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9027fbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(print_statistics(valid_2['label'],marbert_prompt_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13216b5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "875b983a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at UBC-NLP/MARBERTv2 were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer= AutoTokenizer.from_pretrained('UBC-NLP/MARBERTv2')\n",
    "arabert_model=AutoModel.from_pretrained('UBC-NLP/MARBERTv2')\n",
    "pBERT = BERTPromptV1(arabert_model, tokenizer, prompt_length=25)\n",
    "testDataset = BERTPromptDatasetV2(test[['text']], tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3b9d1773",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/750 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "  0%|                                                                                  | 2/750 [00:00<00:43, 17.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 750/750 [00:23<00:00, 31.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 750/750 [00:23<00:00, 31.80it/s]\n"
     ]
    }
   ],
   "source": [
    "model_pth=[\"/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/marbert_prompt/Loss_params_freezing.bin\"]\n",
    "pBERT.load_state_dict(torch.load(model_pth[0]))\n",
    "test_loader = DataLoader(testDataset, batch_size=4, \n",
    "                              num_workers=2, shuffle=False, pin_memory=True)\n",
    "marbert_prompt_predictions=[]\n",
    "# for pth in model_pth:\n",
    "# arabert_prompt.load_state_dict(torch.load(pth))\n",
    "prediction = inference_fn_promptV2(test_loader, pBERT, device)\n",
    "marbert_prompt_predictions.append(prediction)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "predictions_1_tokeepp=np.array(marbert_prompt_predictions)[0]\n",
    "Prediction_final.append(predictions_1_tokeepp)\n",
    "\n",
    "marbert_prompt_predictions=np.argmax(np.array(marbert_prompt_predictions)[0],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0bbcd76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(print_statistics(valid_2['label'],marbert_prompt_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf1d84d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47e7fae9",
   "metadata": {},
   "source": [
    "# prediction Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "224fa99a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.87735265, 0.09837374, 0.0242737 ],\n",
       "        [0.00839029, 0.02771917, 0.9638905 ],\n",
       "        [0.8695026 , 0.10876603, 0.02173138],\n",
       "        ...,\n",
       "        [0.9580188 , 0.02968242, 0.01229879],\n",
       "        [0.01814222, 0.05023643, 0.93162143],\n",
       "        [0.03246466, 0.84353083, 0.12400454]], dtype=float32),\n",
       " array([[0.9827221 , 0.00983881, 0.00743911],\n",
       "        [0.0015404 , 0.00198895, 0.9964706 ],\n",
       "        [0.99473685, 0.00381451, 0.0014486 ],\n",
       "        ...,\n",
       "        [0.9931161 , 0.00419629, 0.00268767],\n",
       "        [0.00146575, 0.00244577, 0.9960885 ],\n",
       "        [0.00602709, 0.8443929 , 0.14958003]], dtype=float32),\n",
       " array([[0.30064368, 0.49442172, 0.20493464],\n",
       "        [0.0251786 , 0.0715175 , 0.9033039 ],\n",
       "        [0.73848224, 0.18349338, 0.07802437],\n",
       "        ...,\n",
       "        [0.51082647, 0.35972765, 0.12944593],\n",
       "        [0.02521017, 0.04792966, 0.9268603 ],\n",
       "        [0.01317233, 0.8149198 , 0.1719078 ]], dtype=float32),\n",
       " array([[0.12814169, 0.37626508, 0.49559328],\n",
       "        [0.05192444, 0.19293226, 0.7551433 ],\n",
       "        [0.66938126, 0.2735795 , 0.05703924],\n",
       "        ...,\n",
       "        [0.50086176, 0.38361216, 0.11552614],\n",
       "        [0.0483901 , 0.22058554, 0.7310244 ],\n",
       "        [0.05882126, 0.6672466 , 0.27393213]], dtype=float32),\n",
       " array([[0.6133284 , 0.10047667, 0.2861949 ],\n",
       "        [0.01795992, 0.01933558, 0.96270454],\n",
       "        [0.88738775, 0.02920972, 0.08340251],\n",
       "        ...,\n",
       "        [0.85324603, 0.07187651, 0.07487747],\n",
       "        [0.02428262, 0.05381793, 0.92189944],\n",
       "        [0.04562536, 0.8097871 , 0.14458752]], dtype=float32))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_2_tokeepp,predictions_3_tokeepp,predictions_1_tokeepp,predictions_4_tokeepp,predictions_5_tokeepp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1ad3eb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_f=[predictions_2_tokeepp,predictions_3_tokeepp]\n",
    "prediction_f=np.argmax(np.mean(np.array(prediction_f),axis=0),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "09354b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(print_statistics(valid_2['label'],prediction_f))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cbfebf",
   "metadata": {},
   "source": [
    "## Prompting Explicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8f7f96b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertTokenizer, BertForMaskedLM,AutoModelForMaskedLM,AutoModel,AutoTokenizer,AutoConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "df284d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME='UBC-NLP/MARBERTv2' # bert-base-chinese\n",
    "DATA_PATH=\"/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/Dataset/\"\n",
    "NUM_WORKERS=10\n",
    "TRAIN_BATCH_SIZE=4\n",
    "\n",
    "train_file=\"NADI2022_Subtask2_TRAIN.tsv\"\n",
    "dev_file=\"NADI2022_Subtask2_DEV.tsv\"\n",
    "test_path='/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/Dataset/NADI2022_Subtask2_DEV.tsv'\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "Inputid_dev,typeids_dev,inputnmask_dev=ProcessDataTest(test_path,tokenizer)\n",
    "valid_dataset = DataLoader(MyDataSetTest(Inputid_dev,  inputnmask_dev , typeids_dev ), TRAIN_BATCH_SIZE,  shuffle=False,num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "34360ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at UBC-NLP/MARBERTv2 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=Bert_Model(MODEL_NAME).to(device)\n",
    "model_pth=[\"/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/marbert_prompt/marbert_prompt_explicit.bin\"]\n",
    "model.load_state_dict(torch.load(model_pth[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f59127a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "marbert_prompt_predictions=[]\n",
    "# for pth in model_pth:\n",
    "# arabert_prompt.load_state_dict(torch.load(pth))\n",
    "prediction = inference_fn_promptV3(valid_dataset, model, device)\n",
    "marbert_prompt_predictions.append(prediction)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "lebelled_prediction=tokenizer.convert_ids_to_tokens(prediction)\n",
    "val_prediction=[]\n",
    "for val in lebelled_prediction:\n",
    "    if val=='':\n",
    "        val_prediction.append(0)\n",
    "    elif val=='':\n",
    "        val_prediction.append(2)\n",
    "    else:\n",
    "        val_prediction.append(1)\n",
    "val_prediction=np.array(val_prediction)\n",
    "Prediction_final.append(np.mean(np.array(val_prediction),axis=0))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f076122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(print_statistics(valid_2['label'],val_prediction))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7333422",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c94b388d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at UBC-NLP/MARBERTv2 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "model=Bert_Model(MODEL_NAME).to(device)\n",
    "model_pth=[\"/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/marbert_prompt/marbert_prompt_explicit_modify_lr.bin\"]\n",
    "model.load_state_dict(torch.load(model_pth[0]))\n",
    "marbert_prompt_predictions=[]\n",
    "# for pth in model_pth:\n",
    "# arabert_prompt.load_state_dict(torch.load(pth))\n",
    "prediction = inference_fn_promptV3(valid_dataset, model, device)\n",
    "marbert_prompt_predictions.append(prediction)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "lebelled_prediction=tokenizer.convert_ids_to_tokens(prediction)\n",
    "val_prediction=[]\n",
    "for val in lebelled_prediction:\n",
    "    if val=='':\n",
    "        val_prediction.append(0)\n",
    "    elif val=='':\n",
    "        val_prediction.append(2)\n",
    "    else:\n",
    "        val_prediction.append(1)\n",
    "val_prediction=np.array(val_prediction)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003c466a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(print_statistics(valid_2['label'],val_prediction))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2395af8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d643ee39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at UBC-NLP/MARBERTv2 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "model=Bert_Model(MODEL_NAME).to(device)\n",
    "model_pth=[\"/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/marbert_prompt/marbert_prompt_explicit_params_augm.bin\"]\n",
    "model.load_state_dict(torch.load(model_pth[0]))\n",
    "marbert_prompt_predictions=[]\n",
    "# for pth in model_pth:\n",
    "# arabert_prompt.load_state_dict(torch.load(pth))\n",
    "prediction = inference_fn_promptV3(valid_dataset, model, device)\n",
    "marbert_prompt_predictions.append(prediction)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "lebelled_prediction=tokenizer.convert_ids_to_tokens(prediction)\n",
    "val_prediction=[]\n",
    "for val in lebelled_prediction:\n",
    "    if val=='':\n",
    "        val_prediction.append(0)\n",
    "    elif val=='':\n",
    "        val_prediction.append(2)\n",
    "    else:\n",
    "        val_prediction.append(1)\n",
    "val_prediction=np.array(val_prediction)\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e398e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(print_statistics(valid_2['label'],val_prediction))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c8672b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5a3ae18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at UBC-NLP/MARBERTv2 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "model=Bert_Model(MODEL_NAME).to(device)\n",
    "model_pth=[\"/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/marbert_prompt/marbert_prompt_explicit_params_grp.bin\"]\n",
    "model.load_state_dict(torch.load(model_pth[0]))\n",
    "marbert_prompt_predictions=[]\n",
    "# for pth in model_pth:\n",
    "# arabert_prompt.load_state_dict(torch.load(pth))\n",
    "prediction = inference_fn_promptV3(valid_dataset, model, device)\n",
    "marbert_prompt_predictions.append(prediction)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "lebelled_prediction=tokenizer.convert_ids_to_tokens(prediction)\n",
    "val_prediction=[]\n",
    "for val in lebelled_prediction:\n",
    "    if val=='':\n",
    "        val_prediction.append(0)\n",
    "    elif val=='':\n",
    "        val_prediction.append(2)\n",
    "    else:\n",
    "        val_prediction.append(1)\n",
    "val_prediction=np.array(val_prediction)\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960a4c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(print_statistics(valid_2['label'],val_prediction))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782d266e",
   "metadata": {},
   "source": [
    "## Marbert with features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66dd024",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length):\n",
    "        self.df = df\n",
    "        self.max_len = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text = df['arabic_original_text_deomji'].values\n",
    "        self.fetures = df[['POS_1', 'POS_2', 'POS_3', 'POS_4',\n",
    "       'Positive Sentiment', 'Negative Sentiment', 'Neutral Sentiment',\n",
    "       'sentiment', 'Blob Polarity', 'Blob Subjectivity',\n",
    "       'positive Sentiment first half', 'negative Sentiment first half',\n",
    "       'Neutral Sentiment first half', 'first half sentiment',\n",
    "       'first half Blob Polarity', 'first half Blob Subjectivity',\n",
    "       'positive Sentiment second half', 'negative Sentiment second half',\n",
    "       'Neutral Sentiment second half', 'second half sentiment',\n",
    "       'second half Blob Polarity', 'second half Blob Subjectivity', 'Topic :',\n",
    "       'contain_emoji', 'count_special_chracter', 'count_question_marks',\n",
    "       'subjectivity', 'count_verbs', 'count_nouns', 'count_pronun',\n",
    "       'count_adjct',  'count_profane']].astype(float).values\n",
    "       \n",
    "#         self.label=df['label'].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.text[index]\n",
    "        # summary = self.summary[index]\n",
    "        inputs_text = self.tokenizer.encode_plus(\n",
    "                                text,\n",
    "                                truncation=True,\n",
    "                                add_special_tokens=True,\n",
    "                                max_length=self.max_len,\n",
    "                                padding='max_length'\n",
    "                            )\n",
    "        \n",
    "                            \n",
    "#         target = self.label[index]\n",
    "        \n",
    "        text_ids = inputs_text['input_ids']\n",
    "        text_mask = inputs_text['attention_mask']\n",
    "        token_type_ids=inputs_text['token_type_ids']\n",
    "       \n",
    "        \n",
    "        \n",
    "        return {\n",
    "            \n",
    "            'input_ids': torch.tensor(text_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(text_mask, dtype=torch.long),\n",
    "            'features': torch.tensor(self.fetures[index], dtype=torch.float),\n",
    "             'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "#             'target': torch.tensor(target, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e48b034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from transformers import AutoModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def init_weights(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv2d') != -1 or classname.find('ConvTranspose2d') != -1:\n",
    "        nn.init.kaiming_uniform_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "class AttentionWithContext(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(AttentionWithContext, self).__init__()\n",
    "\n",
    "        self.attn = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.contx = nn.Linear(hidden_dim, 1, bias=False)\n",
    "        #self.apply(init_weights)\n",
    "    def forward(self, inp):\n",
    "        u = torch.tanh_(self.attn(inp))\n",
    "        a = F.softmax(self.contx(u))\n",
    "        s = (a * inp).sum(1)\n",
    "        return s\n",
    "\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 pretrained_path='aubmindlab/bert-base-arabert'):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "\n",
    "        \n",
    "        self.transformer = AutoModel.from_pretrained(pretrained_path, output_hidden_states=True)\n",
    "\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None):\n",
    "        outputs = self.transformer(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "            , output_hidden_states=True\n",
    "        )\n",
    "        #(output_last_layer, pooled_cls, (output_layers))\n",
    "        #output[0] (8, seqlen=64, 768) cls [8, 768] ( 12 (8, seqlen=64, 768))\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def output_num(self):\n",
    "        return self.transformer.config.hidden_size\n",
    "\n",
    "class ATTClassifier(nn.Module):\n",
    "    def __init__(self, in_feature, class_num=1, dropout_prob=0.2):\n",
    "        super(ATTClassifier, self).__init__()\n",
    "        self.attention = AttentionWithContext(in_feature)\n",
    "\n",
    "        self.Classifier = nn.Sequential(\n",
    "            nn.Linear(2 * in_feature, 512),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, class_num)\n",
    "        )\n",
    "\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        att = self.attention(x[0]) #(X[0] (bs, seqlenght, embedD) att = \\sum_i alpha_i x[0][i]\n",
    "\n",
    "        xx = torch.cat([att, x[1]], 1)\n",
    "\n",
    "        out = self.Classifier(xx)\n",
    "        return out\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        \n",
    "        self.supports_masking = True\n",
    "\n",
    "        self.bias = bias\n",
    "        self.feature_dim = feature_dim\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        \n",
    "        weight = torch.zeros(feature_dim, 1)\n",
    "        nn.init.kaiming_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "        \n",
    "        if bias:\n",
    "            self.b = nn.Parameter(torch.zeros(step_dim))\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        feature_dim = self.feature_dim \n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = torch.mm(\n",
    "            x.contiguous().view(-1, feature_dim), \n",
    "            self.weight\n",
    "        ).view(-1, step_dim)\n",
    "        \n",
    "        if self.bias:\n",
    "            eij = eij + self.b\n",
    "            \n",
    "        eij = torch.tanh(eij)\n",
    "        a = torch.exp(eij)\n",
    "        \n",
    "        if mask is not None:\n",
    "            a = a * mask\n",
    "\n",
    "        a = a / (torch.sum(a, 1, keepdim=True) + 1e-10)\n",
    "\n",
    "        weighted_input = x * torch.unsqueeze(a, -1)\n",
    "        return torch.sum(weighted_input, 1)\n",
    "\n",
    "class WeightedLayerPooling(nn.Module):\n",
    "    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights=None):\n",
    "        super(WeightedLayerPooling, self).__init__()\n",
    "        self.layer_start = layer_start\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.layer_weights = layer_weights if layer_weights is not None \\\n",
    "            else nn.Parameter(\n",
    "            torch.tensor([1] * (num_hidden_layers + 1 - layer_start), dtype=torch.float)\n",
    "        )\n",
    "\n",
    "    def forward(self, all_hidden_states):\n",
    "        all_layer_embedding = all_hidden_states[self.layer_start:, :, :, :]\n",
    "        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n",
    "        weighted_average = (weight_factor * all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n",
    "        return weighted_average\n",
    "\n",
    "class NADIModel(nn.Module):\n",
    "  def __init__(self, pretrained_path='aubmindlab/bert-base-arabert',in_feature=768, class_num=3, dropout_prob=0.2):\n",
    "        super(NADIModel, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(pretrained_path, output_hidden_states=True)\n",
    "#         freeze(self.bert.embeddings)\n",
    "#         freeze(self.bert.encoder.layer[:4])\n",
    "        \n",
    "            \n",
    "        self.hidden_size = self.bert.config.hidden_size\n",
    "        self.LSTM_1 = nn.LSTM(self.hidden_size,self.hidden_size,bidirectional=True)\n",
    "        self.LSTM_2 = nn.LSTM(self.hidden_size,self.hidden_size,bidirectional=True)\n",
    "        self.LSTM_3 = nn.LSTM(self.hidden_size,self.hidden_size,bidirectional=True)\n",
    "        self.LSTM_4 = nn.LSTM(self.hidden_size,self.hidden_size,bidirectional=True)\n",
    "        self.clf_1 = nn.Linear(self.hidden_size*8,32)\n",
    "        self.clf_2=nn.Linear(32,32)\n",
    "        self.clf_final=nn.Linear(64,class_num)\n",
    "  def forward(self, text_id,text_mask,features):\n",
    "    outputs = self.bert(input_ids=text_id,attention_mask=text_mask)\n",
    "    encoded_layers = outputs[2]\n",
    "    encoded_layer_1 = encoded_layers[-1].permute(1, 0, 2)\n",
    "    encoded_layer_2 = encoded_layers[-2].permute(1, 0, 2)\n",
    "    encoded_layer_3 = encoded_layers[-3].permute(1, 0, 2)\n",
    "    encoded_layer_4 = encoded_layers[-4].permute(1, 0, 2)\n",
    "    enc_hiddens, (last_hidden_1, last_cell) = self.LSTM_1(encoded_layer_1)\n",
    "    enc_hiddens, (last_hidden_2, last_cell) = self.LSTM_2(encoded_layer_2)\n",
    "    enc_hiddens, (last_hidden_3, last_cell) = self.LSTM_3(encoded_layer_3)\n",
    "    enc_hiddens, (last_hidden_4, last_cell) = self.LSTM_4(encoded_layer_4)\n",
    "    output_hidden = torch.cat((last_hidden_1[0], last_hidden_1[1],last_hidden_2[0], last_hidden_2[1],last_hidden_3[0], last_hidden_3[1],last_hidden_4[0], last_hidden_4[1]), dim=1)\n",
    "    output_hidden = F.dropout(output_hidden,0.2)\n",
    "    output_hidden = self.clf_1(output_hidden)\n",
    "    out_2=self.clf_2(features)\n",
    "    output = torch.cat((output_hidden,out_2), dim=1)\n",
    "    output=self.clf_final(output)\n",
    "    # output=torch.softmax(output, dim=1)\n",
    "    return output\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01d64ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5643299d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_feat = pd.read_csv('/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/Dataset/valid_subtask2_with_features_modified.csv')\n",
    "# # valid.fillna(0,inplace=True)\n",
    "# tokenizer_marbert= AutoTokenizer.from_pretrained('UBC-NLP/MARBERTv2')\n",
    "# model_pth=['/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/Marbert/Loss_modified_features-0.bin']\n",
    "# model_marbert =  NADIModel('UBC-NLP/MARBERTv2')\n",
    "# device=\"cuda:0\"\n",
    "# testDataset = TestDataset(valid_feat, tokenizer_marbert,max_length=256)\n",
    "# test_loader = DataLoader(testDataset,\n",
    "#                               batch_size =16,\n",
    "#                               shuffle=False,\n",
    "#                               num_workers = 4,\n",
    "#                               pin_memory = True,\n",
    "#                               drop_last=False)\n",
    "\n",
    "# marbert_predictions=[]\n",
    "# for pth in model_pth:\n",
    "#     model_marbert.load_state_dict(torch.load(pth))\n",
    "#     prediction = inference_fn_marbert_features(test_loader, model_marbert, device)\n",
    "#     marbert_predictions.append(prediction)\n",
    "#     torch.cuda.empty_cache()\n",
    "#     gc.collect()\n",
    "\n",
    "# # predictions_5_tokeepp=np.array(marbert_predictions)[0]\n",
    "# # Prediction_final.append(np.mean(np.array(marbert_predictions),axis=0))\n",
    "\n",
    "# marbert_predictions=np.argmax(np.mean(np.array(marbert_predictions),axis=0),axis=1)\n",
    "# print(print_statistics(valid_2['label'],marbert_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5854a33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_feat = pd.read_csv('/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/Dataset/valid_subtask2_with_features_modified.csv')\n",
    "# tokenizer_marbert= AutoTokenizer.from_pretrained('UBC-NLP/MARBERTv2')\n",
    "# model_pth=['/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/Marbert/Loss_modified_features_err_modified-0.bin']\n",
    "# model_marbert =  NADIModel('UBC-NLP/MARBERTv2')\n",
    "# device=\"cuda:0\"\n",
    "# testDataset = TestDataset(valid_feat, tokenizer_marbert,max_length=256)\n",
    "# test_loader = DataLoader(testDataset,\n",
    "#                               batch_size =16,\n",
    "#                               shuffle=False,\n",
    "#                               num_workers = 4,\n",
    "#                               pin_memory = True,\n",
    "#                               drop_last=False)\n",
    "\n",
    "# marbert_predictions=[]\n",
    "# for pth in model_pth:\n",
    "#     model_marbert.load_state_dict(torch.load(pth))\n",
    "#     prediction = inference_fn_marbert_features(test_loader, model_marbert, device)\n",
    "#     marbert_predictions.append(prediction)\n",
    "#     torch.cuda.empty_cache()\n",
    "#     gc.collect()\n",
    "\n",
    "# # predictions_5_tokeepp=np.array(marbert_predictions)[0]\n",
    "# # Prediction_final.append(np.mean(np.array(marbert_predictions),axis=0))\n",
    "\n",
    "# marbert_predictions=np.argmax(np.mean(np.array(marbert_predictions),axis=0),axis=1)\n",
    "# print(print_statistics(valid_2['label'],marbert_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f67a7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ce0ec8b",
   "metadata": {},
   "source": [
    "# Evaluate Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe1e51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # weights=[66.5,71.5,73.5,68.5,58,63.5,67.5,60.5,69,73.5,72.5,70.5,73.5]\n",
    "# # weights=[0,71.5,73.5,68.5,0,63.5,0,0,69,73.5,72.5,70.5,73.5]\n",
    "# weights=[0,0,73.5,0,0,0,0,0,0,0,0,0,73.5]\n",
    "\n",
    "# ratio = np.array(weights)/np.array(weights).sum()\n",
    "# Prediction_finalll=np.argmax(np.mean(np.array(Prediction_final)*ratio,axis=0),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf17814",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(print_statistics(valid_2['label'],Prediction_finalll))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a207eb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc3c65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction_finalll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cc7e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction_final[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3992a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.tensor(np.array(Prediction_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d32368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio=[0.07288297 ,0.05473177, 0.03702773, 0.19201524 ,0.0128733,  0.02339409,\n",
    "#  0.10009777 ,0.00948172, 0.00958741, 0.13362988, 0.21856049, 0.05880574,\n",
    "#  0.00285641 ,0.03702773 ,0.03702773]\n",
    "# Prediction_finalll=np.argmax(np.mean(np.array(Prediction_final)*ratio,axis=0),axis=1)\n",
    "\n",
    "# print(print_statistics(valid_2['label'],Prediction_finalll))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726481e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from scipy.optimize import minimize\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import log_loss\n",
    "# import os\n",
    "\n",
    "\n",
    "# def log_loss_func(weights):\n",
    "#     ''' scipy minimize will pass the weights as a numpy array '''\n",
    "#     # print(final_prediction.shape)\n",
    "#     final_prediction=0\n",
    "#     for weight, prediction in zip(weights, Prediction_final):\n",
    "# #             print(weight)\n",
    "#             if(np.isnan(prediction).any()):\n",
    "#                 continue\n",
    "        \n",
    "#             final_prediction += weight*prediction\n",
    "    \n",
    "# #     for i in range(len (final_prediction)):\n",
    "# #         final_prediction[i]=np.array(final_prediction[i])\n",
    "# #     final_prediction=np.array(final_prediction)\n",
    "# # #     print(final_prediction.tolist())\n",
    "#     return log_loss(valid_2['label'], final_prediction.tolist())\n",
    "    \n",
    "# #the algorithms need a starting value, right not we chose 0.5 for all weights\n",
    "# #its better to choose many random starting points and run minimize a few times\n",
    "# starting_values = np.ones((len(Prediction_final))) *(1/len(Prediction_final))\n",
    "\n",
    "# #adding constraints  and a different solver as suggested by user 16universe\n",
    "# #https://kaggle2.blob.core.windows.net/forum-message-attachments/75655/2393/otto%20model%20weights.pdf?sv=2012-02-12&se=2015-05-03T21%3A22%3A17Z&sr=b&sp=r&sig=rkeA7EJC%2BiQ%2FJ%2BcMpcA4lYQLFh6ubNqs2XAkGtFsAv0%3D\n",
    "# cons = ({'type':'eq','fun':lambda w: 1-sum(w)})\n",
    "# #our weights are bound between 0 and 1\n",
    "# bounds = [(0,1)]*len(Prediction_final)\n",
    "\n",
    "# res = minimize(log_loss_func, starting_values, method='SLSQP', bounds=bounds, constraints=cons)\n",
    "\n",
    "# print('Ensamble Score: {best_score}'.format(best_score=res['fun']))\n",
    "# print('Best Weights: {weights}'.format(weights=res['x']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9d3772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio=[7.96463392e-02 ,1.11174349e-04, 3.38399517e-02, 1.35303574e-01,\n",
    "#  0.00000000e+00 ,5.58983252e-02 ,8.40190762e-02 ,5.91961943e-06,\n",
    "#  6.24377473e-20 ,1.06227329e-01 ,2.69780614e-01, 2.13761960e-01,\n",
    "#  6.08261244e-03, 8.01924011e-03 ,7.30388484e-03]\n",
    "# Prediction_finalll=np.argmax(np.mean(np.array(Prediction_final)*ratio,axis=0),axis=1)\n",
    "\n",
    "# print(print_statistics(valid_2['label'],Prediction_finalll))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9473e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Prediction_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a9a6623f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Prediction_finalll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c3bba295",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio=[2.70970789e-02, 8.11760872e-02, 0.00000000e+00, 2.15842039e-01,\n",
    " 2.97663889e-02, 2.23241653e-21, 1.68605770e-01, 5.66972872e-07,\n",
    " 1.73321020e-06, 1.65764528e-01, 2.29554217e-01, 8.21915903e-02,\n",
    " 1.73427699e-17]\n",
    "Prediction_finalll=np.argmax(np.mean(np.array(Prediction_final)*ratio,axis=0),axis=1)\n",
    "\n",
    "f= open(\"/home/ahmed/Downloads/Imagenet_VLCQ/Nadi/Dataset/rematchka_subtask2_test_4.txt\",\"w+\")\n",
    "for val in np.array(Prediction_finalll):\n",
    "  f.write(verbalizer[val][0]+ '\\n')\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963b5fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio=[2.70970789e-02, 8.11760872e-02, 0.00000000e+00, 2.15842039e-01,\n",
    "#  2.97663889e-02, 2.23241653e-21, 1.68605770e-01, 5.66972872e-07,\n",
    "#  1.73321020e-06, 1.65764528e-01, 2.29554217e-01, 8.21915903e-02,\n",
    "#  1.73427699e-17]\n",
    "# Prediction_finalll=np.argmax(np.mean(np.array(Prediction_final)*ratio,axis=0),axis=1)\n",
    "\n",
    "# print(print_statistics(valid_2['label'],Prediction_finalll))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79181a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946a909a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction_final[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fd4075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd48ab93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cons = ({'type':'eq','fun':lambda w: 1-sum(w)})\n",
    "# #our weights are bound between 0 and 1\n",
    "# bounds = [(0,1)]*len(Prediction_final)\n",
    "\n",
    "# res = minimize(log_loss_func, starting_values, method='Nelder-Mead', bounds=bounds, constraints=cons)\n",
    "\n",
    "# print('Ensamble Score: {best_score}'.format(best_score=res['fun']))\n",
    "# print('Best Weights: {weights}'.format(weights=res['x']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
